{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265f1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e3e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_loading import file_loader, data_types_converter\n",
    "\n",
    "file_path = \"../dataset/\"\n",
    "\n",
    "df_train = file_loader(file_path + \"training/leaf.csv.lz4\")\n",
    "df_test = file_loader(file_path + \"testing/leaf.csv.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61726c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = ['demand', 'destination_current_public_holiday', 'destination_current_school_holiday', \n",
    "                  'destination_days_to_next_public_holiday', 'destination_days_to_next_school_holiday', \n",
    "                  'od_destination_time', 'od_number_of_similar_12_hours', \n",
    "                  'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours', 'od_origin_month', \n",
    "                  'od_origin_time', 'od_origin_week', 'od_origin_weekday', 'od_origin_year', \n",
    "                  'od_travel_time_minutes', 'origin_current_public_holiday', 'origin_current_school_holiday', \n",
    "                  'origin_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', \n",
    "                  'price', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week', 'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ec1be",
   "metadata": {},
   "source": [
    "#### Convertion of data types to float when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_types_converter(df_train, numerical_data)\n",
    "df_test = data_types_converter(df_test, numerical_data)\n",
    "\n",
    "df_train_modified = df_train.copy()\n",
    "df_test_modified = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10bd7d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (632841, 31)\n",
      "test : (32565, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"train :\", df_train_modified.shape)\n",
    "print(\"test :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65671b",
   "metadata": {},
   "source": [
    "# Creation of dataset to predict on : \n",
    "- evaluation is on the sum of all remaining days until departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1f63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_be_used = [-90, -60, -30, -20, -15, -10, -7, -6, -5, -3, -2, -1]\n",
    "static_features = ['departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday', 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'origin_station_name']#13911 elements if groupby on these\n",
    "fewer_static_features = ['destination_station_name', 'origin_station_name', 'departure_date', \n",
    "                         'od_origin_time', 'od_destination_time']#9173 elements if groupby on these\n",
    "#Differences in terms of size of groupby results would need further data exploration \n",
    "\n",
    "changing_features = ['demand', 'price',\n",
    "       'sale_date', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77514e6b",
   "metadata": {},
   "source": [
    "#### groupby realized to create time series based of same travels:\n",
    "- number of results changes when more (apparently static) features are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def563ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [00:07<00:00, 58.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_test = splitting_travels(df_test, fewer_static_features)\n",
    "# print(len(dataset_test), \"different travels in the dataset\")\n",
    "complete_preprocessed_dataset, information_on_travel = extraction_validation_set(dataset_test, days_to_be_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd2f14",
   "metadata": {},
   "source": [
    "## First test for prediction --> baseline with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5a3496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [#'departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',# 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', #'origin_station_name', #'sale_date', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad8c93",
   "metadata": {},
   "source": [
    "#### Prediction of each day separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc387fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9424429301183095\n",
      "mse : 20.161594724004118\n",
      "r2 : 0.6658339286863304\n",
      "std ae : 4.048272494192937\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d7c1",
   "metadata": {},
   "source": [
    "#### Prediction on the sum of remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eace8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.7225636523266\n",
      "mse : 4845.435469710272\n",
      "r2 : 0.8246162245654879\n",
      "std ae : 59.74390275011959\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, \n",
    "                                                                                                        information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dc4e0",
   "metadata": {},
   "source": [
    "### First improvement : categorical features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43469e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import encode_and_bind\n",
    "\n",
    "cat_features = ['destination_station_name', 'origin_station_name']\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_train_modified = encode_and_bind(df_train_modified, feature)\n",
    "    df_test_modified = encode_and_bind(df_test_modified, feature)\n",
    "\n",
    "df_train, df_test = df_train.align(df_test, join='left', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c8324",
   "metadata": {},
   "source": [
    "### Second improvement : add of a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e83eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_modified['od_origin_day'] = pd.to_datetime(df_train_modified.departure_date).dt.day\n",
    "df_test_modified['od_origin_day'] = pd.to_datetime(df_test_modified.departure_date).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0725fba",
   "metadata": {},
   "source": [
    "### Thrird improvement : convertion of cyclic features to adapted ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "162609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import cyclic_features_transform\n",
    "\n",
    "periodic_features_day = ['od_origin_day', 'sale_day'] \n",
    "periodic_features_month = ['od_origin_month', 'sale_month']\n",
    "\n",
    "df_train_modified = cyclic_features_transform(df_train_modified, periodic_features_day, periodic_features_month)\n",
    "df_test_modified = cyclic_features_transform(df_test_modified, periodic_features_day, periodic_features_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50d6f5",
   "metadata": {},
   "source": [
    "## Second test with transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d88f6df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e298ae",
   "metadata": {},
   "source": [
    "#### Each day separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f09a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9437301706533756\n",
      "mse : 20.182970668189608\n",
      "r2 : 0.6654796355172201\n",
      "std ae : 4.050294272257312\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e76c0",
   "metadata": {},
   "source": [
    "#### Sum of all remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b66f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.741220368744514\n",
      "mse : 4846.8009218612815\n",
      "r2 : 0.8245668011122366\n",
      "std ae : 59.744172003753846\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfbaf9",
   "metadata": {},
   "source": [
    "#### --> No real improvement with features engineering\n",
    "Final results with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d442d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(information_on_travel_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ba7cb",
   "metadata": {},
   "source": [
    "### Third test : xgboost better? --> No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba1c720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.741220368744514\n",
      "mse : 4846.8009218612815\n",
      "r2 : 0.8245668011122366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "xgtrain = xgb.DMatrix(np.array(X_train), np.array(y_train))\n",
    "xgval = xgb.DMatrix(np.array(X_val), np.array(y_val))\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xgtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(xgval, \"Validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "best_iteration = model.best_ntree_limit\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(\n",
    "    complete_preprocessed_dataset, features_names, regr, information_on_travel)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "# print(\"with split :\", mean_absolute_error(model.predict(xgtest, ntree_limit=best_iteration), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa314f0",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b627ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212a272",
   "metadata": {},
   "source": [
    "#### Proposition is to only consider the prices of the last 14 days as inputs for this LSTM, and the demand of the last day as target\n",
    "==> A more complex approach would require more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7094f",
   "metadata": {},
   "source": [
    "### Normalization of features : necessary for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "298c0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_demand = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_demand\"] = scaler_demand.fit_transform(np.array(df_train.demand).reshape(-1, 1))\n",
    "scaler_price = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_price\"] = scaler_price.fit_transform(np.array(df_train.price).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13771c0b",
   "metadata": {},
   "source": [
    "### time series separation into multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a48db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_train = splitting_travels(df_train, fewer_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07968681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d06a36f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9173/9173 [02:25<00:00, 62.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences\n",
    "train_inout_seq = create_inout_sequences(dataset_train, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd9717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [(torch.FloatTensor(np.array(df[0])).view(-1), torch.FloatTensor(np.array(df[1])).view(-1)) for df in train_inout_seq]\n",
    "# X_train_data_normalized = torch.stack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68ad9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4545aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c232de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(1, 100)\n",
      "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "160b475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 0.0029091632\n",
      "epoch:   0 loss: 0.0035382335\n",
      "epoch:   0 loss: 0.0033164099\n",
      "epoch:   0 loss: 0.0029336284\n",
      "epoch:   1 loss: 0.0008304812\n",
      "epoch:   1 loss: 0.0009595641\n",
      "epoch:   1 loss: 0.0011312865\n",
      "epoch:   1 loss: 0.0012801239\n",
      "epoch:   2 loss: 0.0004098872\n",
      "epoch:   2 loss: 0.0004176199\n",
      "epoch:   2 loss: 0.0004445682\n",
      "epoch:   2 loss: 0.0004723408\n",
      "epoch:   3 loss: 0.0000027163\n",
      "epoch:   3 loss: 0.0000024778\n",
      "epoch:   3 loss: 0.0000154208\n",
      "epoch:   3 loss: 0.0000355129\n",
      "epoch:   4 loss: 0.0001068461\n",
      "epoch:   4 loss: 0.0000700396\n",
      "epoch:   4 loss: 0.0000423260\n",
      "epoch:   4 loss: 0.0000155010\n",
      "epoch:   5 loss: 0.0015244450\n",
      "epoch:   5 loss: 0.0007985464\n",
      "epoch:   5 loss: 0.0004090526\n",
      "epoch:   5 loss: 0.0000390926\n",
      "epoch:   6 loss: 0.0015436815\n",
      "epoch:   6 loss: 0.0016332605\n",
      "epoch:   6 loss: 0.0016940219\n",
      "epoch:   6 loss: 0.0017512704\n",
      "epoch:   7 loss: 0.0031987415\n",
      "epoch:   7 loss: 0.0029019280\n",
      "epoch:   7 loss: 0.0027565951\n",
      "epoch:   7 loss: 0.0026516076\n",
      "epoch:   8 loss: 0.0001389277\n",
      "epoch:   8 loss: 0.0000109409\n",
      "epoch:   8 loss: 0.0001132685\n",
      "epoch:   8 loss: 0.0002262548\n",
      "epoch:   9 loss: 0.0014825813\n",
      "epoch:   9 loss: 0.0015993437\n",
      "epoch:   9 loss: 0.0016879973\n",
      "epoch:   9 loss: 0.0017532165\n",
      "epoch:  10 loss: 0.0000170107\n",
      "epoch:  10 loss: 0.0000124611\n",
      "epoch:  10 loss: 0.0000083197\n",
      "epoch:  10 loss: 0.0000109886\n",
      "epoch:  11 loss: 0.0000698124\n",
      "epoch:  11 loss: 0.0000690554\n",
      "epoch:  11 loss: 0.0000650825\n",
      "epoch:  11 loss: 0.0000596268\n",
      "epoch:  12 loss: 0.0010052122\n",
      "epoch:  12 loss: 0.0017268616\n",
      "epoch:  12 loss: 0.0018093435\n",
      "epoch:  12 loss: 0.0018299790\n",
      "epoch:  13 loss: 0.0021451192\n",
      "epoch:  13 loss: 0.0022510167\n",
      "epoch:  13 loss: 0.0023972422\n",
      "epoch:  13 loss: 0.0023867476\n",
      "epoch:  14 loss: 0.0002423372\n",
      "epoch:  14 loss: 0.0002505034\n",
      "epoch:  14 loss: 0.0002678052\n",
      "epoch:  14 loss: 0.0002835147\n",
      "epoch:  15 loss: 0.0002722912\n",
      "epoch:  15 loss: 0.0002467141\n",
      "epoch:  15 loss: 0.0002168124\n",
      "epoch:  15 loss: 0.0001870652\n",
      "epoch:  16 loss: 0.0000437249\n",
      "epoch:  16 loss: 0.0001118394\n",
      "epoch:  16 loss: 0.0001859305\n",
      "epoch:  16 loss: 0.0002545193\n",
      "epoch:  17 loss: 0.0003962601\n",
      "epoch:  17 loss: 0.0001247696\n",
      "epoch:  17 loss: 0.0001268367\n",
      "epoch:  17 loss: 0.0001440764\n",
      "epoch:  18 loss: 0.0000650459\n",
      "epoch:  18 loss: 0.0000672467\n",
      "epoch:  18 loss: 0.0000675618\n",
      "epoch:  18 loss: 0.0000671861\n",
      "epoch:  19 loss: 0.0104153594\n",
      "epoch:  19 loss: 0.0135054812\n",
      "epoch:  19 loss: 0.0151373949\n",
      "epoch:  19 loss: 0.0151300328\n",
      "epoch:  20 loss: 0.0009407687\n",
      "epoch:  20 loss: 0.0008845832\n",
      "epoch:  20 loss: 0.0008715944\n",
      "epoch:  20 loss: 0.0008925358\n",
      "epoch:  21 loss: 0.0012336120\n",
      "epoch:  21 loss: 0.0012617436\n",
      "epoch:  21 loss: 0.0013008119\n",
      "epoch:  21 loss: 0.0013498893\n",
      "epoch:  22 loss: 0.0028394123\n",
      "epoch:  22 loss: 0.0031288604\n",
      "epoch:  22 loss: 0.0033419488\n",
      "epoch:  22 loss: 0.0035020986\n",
      "epoch:  23 loss: 0.0007272764\n",
      "epoch:  23 loss: 0.0007268135\n",
      "epoch:  23 loss: 0.0007232986\n",
      "epoch:  23 loss: 0.0007179608\n",
      "epoch:  24 loss: 0.0000717076\n",
      "epoch:  24 loss: 0.0000575348\n",
      "epoch:  24 loss: 0.0000852659\n",
      "epoch:  24 loss: 0.0000721544\n",
      "epoch:  25 loss: 0.0000555618\n",
      "epoch:  25 loss: 0.0000581722\n",
      "epoch:  25 loss: 0.0000591730\n",
      "epoch:  25 loss: 0.0000595541\n",
      "epoch:  26 loss: 0.0000191984\n",
      "epoch:  26 loss: 0.0000052041\n",
      "epoch:  26 loss: 0.0000017702\n",
      "epoch:  26 loss: 0.0000007413\n",
      "epoch:  27 loss: 0.0000940403\n",
      "epoch:  27 loss: 0.0001102991\n",
      "epoch:  27 loss: 0.0001264262\n",
      "epoch:  27 loss: 0.0001417620\n",
      "epoch:  28 loss: 0.0016181776\n",
      "epoch:  28 loss: 0.0015408258\n",
      "epoch:  28 loss: 0.0014747564\n",
      "epoch:  28 loss: 0.0014202293\n",
      "epoch:  29 loss: 0.0009193198\n",
      "epoch:  29 loss: 0.0008655936\n",
      "epoch:  29 loss: 0.0008395789\n",
      "epoch:  29 loss: 0.0008220551\n",
      "epoch:  30 loss: 0.0003395505\n",
      "epoch:  30 loss: 0.0003239230\n",
      "epoch:  30 loss: 0.0003131556\n",
      "epoch:  30 loss: 0.0003042313\n",
      "epoch:  31 loss: 0.0144827319\n",
      "epoch:  31 loss: 0.0137563199\n",
      "epoch:  31 loss: 0.0134199867\n",
      "epoch:  31 loss: 0.0131594175\n",
      "epoch:  32 loss: 0.0033673781\n",
      "epoch:  32 loss: 0.0027611661\n",
      "epoch:  32 loss: 0.0022576444\n",
      "epoch:  32 loss: 0.0022569818\n",
      "epoch:  33 loss: 0.0014970523\n",
      "epoch:  33 loss: 0.0014751960\n",
      "epoch:  33 loss: 0.0014408207\n",
      "epoch:  33 loss: 0.0014097990\n",
      "epoch:  34 loss: 0.0000012104\n",
      "epoch:  34 loss: 0.0000000014\n",
      "epoch:  34 loss: 0.0000019248\n",
      "epoch:  34 loss: 0.0000014269\n",
      "epoch:  35 loss: 0.0005715763\n",
      "epoch:  35 loss: 0.0006022792\n",
      "epoch:  35 loss: 0.0006504780\n",
      "epoch:  35 loss: 0.0006603048\n",
      "epoch:  36 loss: 0.0168160312\n",
      "epoch:  36 loss: 0.0167433769\n",
      "epoch:  36 loss: 0.0167276617\n",
      "epoch:  36 loss: 0.0167322252\n",
      "epoch:  37 loss: 0.0001090420\n",
      "epoch:  37 loss: 0.0000791525\n",
      "epoch:  37 loss: 0.0000715582\n",
      "epoch:  37 loss: 0.0000693460\n",
      "epoch:  38 loss: 0.0092619620\n",
      "epoch:  38 loss: 0.0106246332\n",
      "epoch:  38 loss: 0.0110219754\n",
      "epoch:  38 loss: 0.0111758076\n",
      "epoch:  39 loss: 0.0048190178\n",
      "epoch:  39 loss: 0.0046596248\n",
      "epoch:  39 loss: 0.0045921272\n",
      "epoch:  39 loss: 0.0045387005\n",
      "epoch:  40 loss: 0.0001778362\n",
      "epoch:  40 loss: 0.0001528884\n",
      "epoch:  40 loss: 0.0001323748\n",
      "epoch:  40 loss: 0.0001091179\n",
      "epoch:  41 loss: 0.0098454915\n",
      "epoch:  41 loss: 0.0084287236\n",
      "epoch:  41 loss: 0.0075861244\n",
      "epoch:  41 loss: 0.0070402059\n",
      "epoch:  42 loss: 0.0000001390\n",
      "epoch:  42 loss: 0.0000000012\n",
      "epoch:  42 loss: 0.0000000238\n",
      "epoch:  42 loss: 0.0000000212\n",
      "epoch:  43 loss: 0.0000002826\n",
      "epoch:  43 loss: 0.0000104389\n",
      "epoch:  43 loss: 0.0000205550\n",
      "epoch:  43 loss: 0.0000240577\n",
      "epoch:  44 loss: 0.0000061110\n",
      "epoch:  44 loss: 0.0000001406\n",
      "epoch:  44 loss: 0.0000002050\n",
      "epoch:  44 loss: 0.0000005317\n",
      "epoch:  45 loss: 0.0000030625\n",
      "epoch:  45 loss: 0.0000042519\n",
      "epoch:  45 loss: 0.0000064258\n",
      "epoch:  45 loss: 0.0000089480\n",
      "epoch:  46 loss: 0.0050963494\n",
      "epoch:  46 loss: 0.0056034359\n",
      "epoch:  46 loss: 0.0057282140\n",
      "epoch:  46 loss: 0.0057569407\n",
      "epoch:  47 loss: 0.0224581882\n",
      "epoch:  47 loss: 0.0258581862\n",
      "epoch:  47 loss: 0.0246743113\n",
      "epoch:  47 loss: 0.0243983250\n",
      "epoch:  48 loss: 0.0000395516\n",
      "epoch:  48 loss: 0.0000729923\n",
      "epoch:  48 loss: 0.0001028027\n",
      "epoch:  48 loss: 0.0001271416\n",
      "epoch:  49 loss: 0.0002718625\n",
      "epoch:  49 loss: 0.0002860555\n",
      "epoch:  49 loss: 0.0002962521\n",
      "epoch:  49 loss: 0.0003164996\n",
      "epoch:  50 loss: 0.0030117384\n",
      "epoch:  50 loss: 0.0029535941\n",
      "epoch:  50 loss: 0.0029088804\n",
      "epoch:  50 loss: 0.0028754475\n",
      "epoch:  51 loss: 0.0051294141\n",
      "epoch:  51 loss: 0.0058087762\n",
      "epoch:  51 loss: 0.0061396016\n",
      "epoch:  51 loss: 0.0063064997\n",
      "epoch:  52 loss: 0.0001459167\n",
      "epoch:  52 loss: 0.0001647440\n",
      "epoch:  52 loss: 0.0001621547\n",
      "epoch:  52 loss: 0.0001509167\n",
      "epoch:  53 loss: 0.0001976909\n",
      "epoch:  53 loss: 0.0001086949\n",
      "epoch:  53 loss: 0.0000869673\n",
      "epoch:  53 loss: 0.0000727470\n",
      "epoch:  54 loss: 0.0013393239\n",
      "epoch:  54 loss: 0.0013469345\n",
      "epoch:  54 loss: 0.0014143010\n",
      "epoch:  54 loss: 0.0014730950\n",
      "epoch:  55 loss: 0.0011783573\n",
      "epoch:  55 loss: 0.0011488762\n",
      "epoch:  55 loss: 0.0011638750\n",
      "epoch:  55 loss: 0.0011877469\n",
      "epoch:  56 loss: 0.0007201089\n",
      "epoch:  56 loss: 0.0007525748\n",
      "epoch:  56 loss: 0.0007888071\n",
      "epoch:  56 loss: 0.0008215220\n",
      "epoch:  57 loss: 0.0005850671\n",
      "epoch:  57 loss: 0.0006598086\n",
      "epoch:  57 loss: 0.0007192391\n",
      "epoch:  57 loss: 0.0007720991\n",
      "epoch:  58 loss: 0.0019345080\n",
      "epoch:  58 loss: 0.0018415835\n",
      "epoch:  58 loss: 0.0018192347\n",
      "epoch:  58 loss: 0.0018243585\n",
      "epoch:  59 loss: 0.0284387320\n",
      "epoch:  59 loss: 0.0284371637\n",
      "epoch:  59 loss: 0.0284776650\n",
      "epoch:  59 loss: 0.0285177119\n",
      "epoch:  60 loss: 0.0007388894\n",
      "epoch:  60 loss: 0.0007768864\n",
      "epoch:  60 loss: 0.0007948854\n",
      "epoch:  60 loss: 0.0008006194\n",
      "epoch:  61 loss: 0.0075560850\n",
      "epoch:  61 loss: 0.0075617130\n",
      "epoch:  61 loss: 0.0075261984\n",
      "epoch:  61 loss: 0.0074805054\n",
      "epoch:  62 loss: 0.0002770029\n",
      "epoch:  62 loss: 0.0002698320\n",
      "epoch:  62 loss: 0.0002640630\n",
      "epoch:  62 loss: 0.0002567263\n",
      "epoch:  63 loss: 0.0002463416\n",
      "epoch:  63 loss: 0.0002319075\n",
      "epoch:  63 loss: 0.0002571792\n",
      "epoch:  63 loss: 0.0003385429\n",
      "epoch:  64 loss: 0.0000641147\n",
      "epoch:  64 loss: 0.0001130783\n",
      "epoch:  64 loss: 0.0001516483\n",
      "epoch:  64 loss: 0.0001835778\n",
      "epoch:  65 loss: 0.0001476179\n",
      "epoch:  65 loss: 0.0001512845\n",
      "epoch:  65 loss: 0.0001457468\n",
      "epoch:  65 loss: 0.0001396128\n",
      "epoch:  66 loss: 0.0010004141\n",
      "epoch:  66 loss: 0.0010508734\n",
      "epoch:  66 loss: 0.0010585234\n",
      "epoch:  66 loss: 0.0010554887\n",
      "epoch:  67 loss: 0.0000052387\n",
      "epoch:  67 loss: 0.0000024536\n",
      "epoch:  67 loss: 0.0000004590\n",
      "epoch:  67 loss: 0.0000000033\n",
      "epoch:  68 loss: 0.0000491775\n",
      "epoch:  68 loss: 0.0000545173\n",
      "epoch:  68 loss: 0.0000576940\n",
      "epoch:  68 loss: 0.0000619822\n",
      "epoch:  69 loss: 0.0013803857\n",
      "epoch:  69 loss: 0.0012837905\n",
      "epoch:  69 loss: 0.0012392036\n",
      "epoch:  69 loss: 0.0012006451\n",
      "epoch:  70 loss: 0.0000072090\n",
      "epoch:  70 loss: 0.0000124510\n",
      "epoch:  70 loss: 0.0000167111\n",
      "epoch:  70 loss: 0.0000210629\n",
      "epoch:  71 loss: 0.0005508318\n",
      "epoch:  71 loss: 0.0005340516\n",
      "epoch:  71 loss: 0.0005250429\n",
      "epoch:  71 loss: 0.0005194391\n",
      "epoch:  72 loss: 0.0000817701\n",
      "epoch:  72 loss: 0.0000569935\n",
      "epoch:  72 loss: 0.0000512452\n",
      "epoch:  72 loss: 0.0000494453\n",
      "epoch:  73 loss: 0.0000066192\n",
      "epoch:  73 loss: 0.0000112994\n",
      "epoch:  73 loss: 0.0000118047\n",
      "epoch:  73 loss: 0.0000109602\n",
      "epoch:  74 loss: 0.0040558325\n",
      "epoch:  74 loss: 0.0040216651\n",
      "epoch:  74 loss: 0.0040703458\n",
      "epoch:  74 loss: 0.0042351023\n",
      "epoch:  75 loss: 0.0004807227\n",
      "epoch:  75 loss: 0.0004582855\n",
      "epoch:  75 loss: 0.0004288223\n",
      "epoch:  75 loss: 0.0003971006\n",
      "epoch:  76 loss: 0.0001581794\n",
      "epoch:  76 loss: 0.0001540743\n",
      "epoch:  76 loss: 0.0001525864\n",
      "epoch:  76 loss: 0.0001516557\n",
      "epoch:  77 loss: 0.0037331744\n",
      "epoch:  77 loss: 0.0037867690\n",
      "epoch:  77 loss: 0.0038143133\n",
      "epoch:  77 loss: 0.0038284697\n",
      "epoch:  78 loss: 0.0003793911\n",
      "epoch:  78 loss: 0.0003198082\n",
      "epoch:  78 loss: 0.0002902402\n",
      "epoch:  78 loss: 0.0002673664\n",
      "epoch:  79 loss: 0.0340129733\n",
      "epoch:  79 loss: 0.0341412053\n",
      "epoch:  79 loss: 0.0343055427\n",
      "epoch:  79 loss: 0.0344809927\n",
      "epoch:  80 loss: 0.0027955684\n",
      "epoch:  80 loss: 0.0029639886\n",
      "epoch:  80 loss: 0.0030244829\n",
      "epoch:  80 loss: 0.0030522253\n",
      "epoch:  81 loss: 0.0013490485\n",
      "epoch:  81 loss: 0.0012775619\n",
      "epoch:  81 loss: 0.0012170789\n",
      "epoch:  81 loss: 0.0011647292\n",
      "epoch:  82 loss: 0.0003254911\n",
      "epoch:  82 loss: 0.0003009383\n",
      "epoch:  82 loss: 0.0003012362\n",
      "epoch:  82 loss: 0.0002965784\n",
      "epoch:  83 loss: 0.0015531336\n",
      "epoch:  83 loss: 0.0015654007\n",
      "epoch:  83 loss: 0.0016139460\n",
      "epoch:  83 loss: 0.0016511442\n",
      "epoch:  84 loss: 0.0281685255\n",
      "epoch:  84 loss: 0.0279979408\n",
      "epoch:  84 loss: 0.0280946065\n",
      "epoch:  84 loss: 0.0281899590\n",
      "epoch:  85 loss: 0.0045157606\n",
      "epoch:  85 loss: 0.0044278577\n",
      "epoch:  85 loss: 0.0043390752\n",
      "epoch:  85 loss: 0.0042407671\n",
      "epoch:  86 loss: 0.0009552080\n",
      "epoch:  86 loss: 0.0009839563\n",
      "epoch:  86 loss: 0.0010147174\n",
      "epoch:  86 loss: 0.0010429162\n",
      "epoch:  87 loss: 0.0039739762\n",
      "epoch:  87 loss: 0.0040894654\n",
      "epoch:  87 loss: 0.0041512325\n",
      "epoch:  87 loss: 0.0042324029\n",
      "epoch:  88 loss: 0.0004909200\n",
      "epoch:  88 loss: 0.0005502668\n",
      "epoch:  88 loss: 0.0005560986\n",
      "epoch:  88 loss: 0.0005396032\n",
      "epoch:  89 loss: 0.0000297325\n",
      "epoch:  89 loss: 0.0000251875\n",
      "epoch:  89 loss: 0.0000234893\n",
      "epoch:  89 loss: 0.0000223052\n",
      "epoch:  90 loss: 0.0007468236\n",
      "epoch:  90 loss: 0.0008287987\n",
      "epoch:  90 loss: 0.0009223222\n",
      "epoch:  90 loss: 0.0010790890\n",
      "epoch:  91 loss: 0.0010202425\n",
      "epoch:  91 loss: 0.0009165388\n",
      "epoch:  91 loss: 0.0009072477\n",
      "epoch:  91 loss: 0.0009052846\n",
      "epoch:  92 loss: 0.0005097231\n",
      "epoch:  92 loss: 0.0004903259\n",
      "epoch:  92 loss: 0.0004594244\n",
      "epoch:  92 loss: 0.0004478619\n",
      "epoch:  93 loss: 0.0008841578\n",
      "epoch:  93 loss: 0.0011208617\n",
      "epoch:  93 loss: 0.0006604151\n",
      "epoch:  93 loss: 0.0008594493\n",
      "epoch:  94 loss: 0.0015297557\n",
      "epoch:  94 loss: 0.0015419022\n",
      "epoch:  94 loss: 0.0015583716\n",
      "epoch:  94 loss: 0.0015675239\n",
      "epoch:  95 loss: 0.0015593412\n",
      "epoch:  95 loss: 0.0016925846\n",
      "epoch:  95 loss: 0.0017575019\n",
      "epoch:  95 loss: 0.0017823268\n",
      "epoch:  96 loss: 0.0000018271\n",
      "epoch:  96 loss: 0.0000010858\n",
      "epoch:  96 loss: 0.0000050811\n",
      "epoch:  96 loss: 0.0000098198\n",
      "epoch:  97 loss: 0.0017976389\n",
      "epoch:  97 loss: 0.0015197570\n",
      "epoch:  97 loss: 0.0014600341\n",
      "epoch:  97 loss: 0.0013496003\n",
      "epoch:  98 loss: 0.0010330075\n",
      "epoch:  98 loss: 0.0011723209\n",
      "epoch:  98 loss: 0.0012233544\n",
      "epoch:  98 loss: 0.0012214037\n",
      "epoch:  99 loss: 0.0005904484\n",
      "epoch:  99 loss: 0.0004904685\n",
      "epoch:  99 loss: 0.0004736293\n",
      "epoch:  99 loss: 0.0004758943\n",
      "epoch:  99 loss: 0.000475894311\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch = random.choices(list_of_tensors, k=batch_size)\n",
    "    for k in range(4):\n",
    "    \n",
    "        for seq, labels in batch:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "\n",
    "    #     if i%25 == 1:\n",
    "        \n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.12f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef9a32",
   "metadata": {},
   "source": [
    "#### Adaptation of the validation dataset to LSTM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "156e60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5004/5004 [00:37<00:00, 133.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences_validation\n",
    "all_validation_data = []; all_validation_target = []\n",
    "for ts in tqdm(complete_preprocessed_dataset):\n",
    "    available_part, unavailable_part = ts\n",
    "    if len(available_part) > train_window: #prediction possible\n",
    "        limit = len(available_part)\n",
    "        whole_df = pd.concat([available_part.iloc[len(available_part)-train_window:], unavailable_part])\n",
    "        features_val, target_val = create_inout_sequences_validation(whole_df, train_window, scaler_price)\n",
    "        all_validation_data.extend(features_val)\n",
    "        all_validation_target.extend(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a76c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors_test = [(torch.FloatTensor(np.array(df)).view(-1)) for df in all_validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6776f6",
   "metadata": {},
   "source": [
    "### Predictions on validation data --> test on single demand's day each time (no sum until departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfae13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 51310/51310 [00:49<00:00, 1032.77it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_inputs = []\n",
    "kept_targets = []\n",
    "for i in tqdm(range(len(list_of_tensors_test))):\n",
    "    seq = torch.FloatTensor(list_of_tensors_test[i])\n",
    "    if len(seq)>0:\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            kept_targets.append(all_validation_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52742702",
   "metadata": {},
   "source": [
    "#### To inverse previous scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6636a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51310, 1)\n",
      "(51310,)\n"
     ]
    }
   ],
   "source": [
    "actual_predictions = scaler_demand.inverse_transform(np.array(test_inputs).reshape(-1, 1))\n",
    "print(actual_predictions.shape)\n",
    "print(np.array(kept_targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0d310",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cc2159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 7.946596261423899\n",
      "mse : 218.45434511714237\n",
      "r2 : -0.13107101450863845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(kept_targets, actual_predictions))\n",
    "print(\"mse :\", mean_squared_error(kept_targets, actual_predictions))\n",
    "print(\"r2 :\", r2_score(kept_targets, actual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3779ce",
   "metadata": {},
   "source": [
    "### Conclusion on deep learning approach : \n",
    "- does not work well, no convergence (seen during the training), then performances remain bad with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7a7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
