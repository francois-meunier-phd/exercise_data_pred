{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c632f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8c7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_loading import file_loader, data_types_converter\n",
    "\n",
    "file_path = \"../dataset/\"\n",
    "\n",
    "df_train = file_loader(file_path + \"training/leaf.csv.lz4\")\n",
    "df_test = file_loader(file_path + \"testing/leaf.csv.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b826de",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = ['demand', 'destination_current_public_holiday', 'destination_current_school_holiday', \n",
    "                  'destination_days_to_next_public_holiday', 'destination_days_to_next_school_holiday', \n",
    "                  'od_destination_time', 'od_number_of_similar_12_hours', \n",
    "                  'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours', 'od_origin_month', \n",
    "                  'od_origin_time', 'od_origin_week', 'od_origin_weekday', 'od_origin_year', \n",
    "                  'od_travel_time_minutes', 'origin_current_public_holiday', 'origin_current_school_holiday', \n",
    "                  'origin_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', \n",
    "                  'price', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week', 'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9017511",
   "metadata": {},
   "source": [
    "#### Convertion of data types to float when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b5c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_types_converter(df_train, numerical_data)\n",
    "df_test = data_types_converter(df_test, numerical_data)\n",
    "\n",
    "df_train_modified = df_train.copy()\n",
    "df_test_modified = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34927265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (632841, 31)\n",
      "test : (32565, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"train :\", df_train_modified.shape)\n",
    "print(\"test :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9fc27",
   "metadata": {},
   "source": [
    "# Creation of dataset to predict on : \n",
    "- evaluation is on the sum of all remaining days until departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefd7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_be_used = [-90, -60, -30, -20, -15, -10, -7, -6, -5, -3, -2, -1]\n",
    "static_features = ['departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday', 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'origin_station_name']#13911 elements if groupby on these\n",
    "fewer_static_features = ['destination_station_name', 'origin_station_name', 'departure_date', \n",
    "                         'od_origin_time', 'od_destination_time']#9173 elements if groupby on these\n",
    "#Differences in terms of size of groupby results would need further data exploration \n",
    "\n",
    "changing_features = ['demand', 'price',\n",
    "       'sale_date', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c60fd4",
   "metadata": {},
   "source": [
    "#### groupby realized to create time series based of same travels:\n",
    "- number of results changes when more (apparently static) features are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc248901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [00:07<00:00, 57.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_test = splitting_travels(df_test, fewer_static_features)\n",
    "# print(len(dataset_test), \"different travels in the dataset\")\n",
    "complete_preprocessed_dataset, information_on_travel = extraction_validation_set(dataset_test, days_to_be_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdd446",
   "metadata": {},
   "source": [
    "## First test for prediction --> baseline with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3aab95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [#'departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',# 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', #'origin_station_name', #'sale_date', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20328f",
   "metadata": {},
   "source": [
    "#### Prediction of each day separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1654a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9420217021261619\n",
      "mse : 20.164198690441026\n",
      "r2 : 0.6657907695391516\n",
      "std mae : 4.048796166629468\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6281a",
   "metadata": {},
   "source": [
    "#### Prediction on the sum of remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b53157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.68107989464443\n",
      "mse : 4834.850087796312\n",
      "r2 : 0.8249993695389566\n",
      "std mae : 59.680068912060705\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, \n",
    "                                                                                                        information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3b4b0",
   "metadata": {},
   "source": [
    "### First improvement : categorical features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0592372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import encode_and_bind\n",
    "\n",
    "cat_features = ['destination_station_name', 'origin_station_name']\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_train_modified = encode_and_bind(df_train_modified, feature)\n",
    "    df_test_modified = encode_and_bind(df_test_modified, feature)\n",
    "\n",
    "df_train, df_test = df_train.align(df_test, join='left', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f4c3c",
   "metadata": {},
   "source": [
    "### Second improvement : add of a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba62948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_modified['od_origin_day'] = pd.to_datetime(df_train_modified.departure_date).dt.day\n",
    "df_test_modified['od_origin_day'] = pd.to_datetime(df_test_modified.departure_date).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e26daa",
   "metadata": {},
   "source": [
    "### Thrird improvement : convertion of cyclic features to adapted ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6843708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import cyclic_features_transform\n",
    "\n",
    "periodic_features_day = ['od_origin_day', 'sale_day'] \n",
    "periodic_features_month = ['od_origin_month', 'sale_month']\n",
    "\n",
    "df_train_modified = cyclic_features_transform(df_train_modified, periodic_features_day, periodic_features_month)\n",
    "df_test_modified = cyclic_features_transform(df_test_modified, periodic_features_day, periodic_features_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e985a39",
   "metadata": {},
   "source": [
    "## Second test with transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158f853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2200e2",
   "metadata": {},
   "source": [
    "#### Each day separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4046658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9416716920485655\n",
      "mse : 20.12559822764714\n",
      "r2 : 0.6664305485437045\n",
      "std mae : 4.044194514108561\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0d0dd",
   "metadata": {},
   "source": [
    "#### Sum of all remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02a1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.63564530289728\n",
      "mse : 4829.511413520632\n",
      "r2 : 0.8251926064226458\n",
      "std mae : 59.6624856787473\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d40a6a",
   "metadata": {},
   "source": [
    "#### --> No real improvement with features engineering\n",
    "Final results with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41fabe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(information_on_travel_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ef709",
   "metadata": {},
   "source": [
    "### Third test : xgboost better? --> No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72b543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.63564530289728\n",
      "mse : 4829.511413520632\n",
      "r2 : 0.8251926064226458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "xgtrain = xgb.DMatrix(np.array(X_train), np.array(y_train))\n",
    "xgval = xgb.DMatrix(np.array(X_val), np.array(y_val))\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xgtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(xgval, \"Validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "best_iteration = model.best_ntree_limit\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(\n",
    "    complete_preprocessed_dataset, features_names, regr, information_on_travel)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "# print(\"with split :\", mean_absolute_error(model.predict(xgtest, ntree_limit=best_iteration), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e9da1",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa6d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339dafd",
   "metadata": {},
   "source": [
    "#### Proposition is to only consider the prices of the last 14 days as inputs for this LSTM, and the demand of the last day as target\n",
    "==> A more complex approach would require more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40729af2",
   "metadata": {},
   "source": [
    "### Normalization of features : necessary for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "532d794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_demand = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_demand\"] = scaler_demand.fit_transform(np.array(df_train.demand).reshape(-1, 1))\n",
    "scaler_price = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_price\"] = scaler_price.fit_transform(np.array(df_train.price).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080060f",
   "metadata": {},
   "source": [
    "### time series separation into multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518f95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_train = splitting_travels(df_train, fewer_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b711f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████▏ | 8959/9173 [02:20<00:03, 61.41it/s]"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences\n",
    "train_inout_seq = create_inout_sequences(dataset_train, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36085495",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [(torch.FloatTensor(np.array(df[0])).view(-1), torch.FloatTensor(np.array(df[1])).view(-1)) for df in train_inout_seq]\n",
    "# X_train_data_normalized = torch.stack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch = random.choices(list_of_tensors, k=batch_size)\n",
    "    for k in range(4):\n",
    "    \n",
    "        for seq, labels in batch:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "\n",
    "    #     if i%25 == 1:\n",
    "        \n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.12f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72670d61",
   "metadata": {},
   "source": [
    "#### Adaptation of the validation dataset to LSTM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12be529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import create_inout_sequences_validation\n",
    "all_validation_data = []; all_validation_target = []\n",
    "for ts in tqdm(complete_preprocessed_dataset):\n",
    "    available_part, unavailable_part = ts\n",
    "    if len(available_part) > train_window: #prediction possible\n",
    "        limit = len(available_part)\n",
    "        whole_df = pd.concat([available_part.iloc[len(available_part)-train_window:], unavailable_part])\n",
    "        features_val, target_val = create_inout_sequences_validation(whole_df, train_window)\n",
    "        all_validation_data.extend(features_val)\n",
    "        all_validation_target.extend(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors_test = [(torch.FloatTensor(np.array(df)).view(-1)) for df in all_validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96758f",
   "metadata": {},
   "source": [
    "### Predictions on validation data --> test on single demand's day each time (no sum until departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_inputs = []\n",
    "kept_targets = []\n",
    "for i in tqdm(range(len(list_of_tensors_test))):\n",
    "    seq = torch.FloatTensor(list_of_tensors_test[i])\n",
    "    if len(seq)>0:\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            kept_targets.append(all_validation_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5044c9a",
   "metadata": {},
   "source": [
    "#### To inverse previous scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = scaler_demand.inverse_transform(np.array(test_inputs).reshape(-1, 1))\n",
    "print(actual_predictions.shape)\n",
    "print(np.array(kept_targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1b5d",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(kept_targets, actual_predictions))\n",
    "print(\"mse :\", mean_squared_error(kept_targets, actual_predictions))\n",
    "print(\"r2 :\", r2_score(kept_targets, actual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80068a8c",
   "metadata": {},
   "source": [
    "### Conclusion on deep learning approach : \n",
    "- does not work well, no convergence (seen during the training), then performances remain bad with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbcafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
