{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3438ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b070d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_loading import file_loader, data_types_converter\n",
    "\n",
    "file_path = \"../dataset/\"\n",
    "\n",
    "df_train = file_loader(file_path + \"training/leaf.csv.lz4\")\n",
    "df_test = file_loader(file_path + \"testing/leaf.csv.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bc1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = ['demand', 'destination_current_public_holiday', 'destination_current_school_holiday', \n",
    "                  'destination_days_to_next_public_holiday', 'destination_days_to_next_school_holiday', \n",
    "                  'od_destination_time', 'od_number_of_similar_12_hours', \n",
    "                  'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours', 'od_origin_month', \n",
    "                  'od_origin_time', 'od_origin_week', 'od_origin_weekday', 'od_origin_year', \n",
    "                  'od_travel_time_minutes', 'origin_current_public_holiday', 'origin_current_school_holiday', \n",
    "                  'origin_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', \n",
    "                  'price', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week', 'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb038994",
   "metadata": {},
   "source": [
    "#### Convertion of data types to float when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfeae982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_types_converter(df_train, numerical_data)\n",
    "df_test = data_types_converter(df_test, numerical_data)\n",
    "\n",
    "df_train_modified = df_train.copy()\n",
    "df_test_modified = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5eca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (632841, 31)\n",
      "test : (32565, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"train :\", df_train_modified.shape)\n",
    "print(\"test :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea9bbd",
   "metadata": {},
   "source": [
    "# Creation of dataset to predict on : \n",
    "- evaluation is on the sum of all remaining days until departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bee9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_be_used = [-90, -60, -30, -20, -15, -10, -7, -6, -5, -3, -2, -1]\n",
    "static_features = ['departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday', 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'origin_station_name']#13911 elements if groupby on these\n",
    "fewer_static_features = ['destination_station_name', 'origin_station_name', 'departure_date', \n",
    "                         'od_origin_time', 'od_destination_time']#9173 elements if groupby on these\n",
    "#Differences in terms of size of groupby results would need further data exploration \n",
    "\n",
    "changing_features = ['demand', 'price',\n",
    "       'sale_date', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e5b24",
   "metadata": {},
   "source": [
    "#### groupby realized to create time series based of same travels:\n",
    "- number of results changes when more (apparently static) features are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d3bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [00:22<00:00, 18.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_test = splitting_travels(df_test, fewer_static_features)\n",
    "# print(len(dataset_test), \"different travels in the dataset\")\n",
    "complete_preprocessed_dataset, information_on_travel = extraction_validation_set(dataset_test, days_to_be_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712285c",
   "metadata": {},
   "source": [
    "## First test for prediction --> baseline with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9257177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's only consider numerical features\n",
    "features_names = [#'departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',# 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', #'origin_station_name', #'sale_date', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23de5dd",
   "metadata": {},
   "source": [
    "#### Prediction of each day separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312de0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9422947216326971\n",
      "mse : 20.178494562962864\n",
      "r2 : 0.6655538242170171\n",
      "std ae : 4.050430320012616\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cc71f",
   "metadata": {},
   "source": [
    "#### Prediction on the sum of remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfde46f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.682396839332746\n",
      "mse : 4830.855355575066\n",
      "r2 : 0.8251439615417171\n",
      "std ae : 59.64580380358238\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, \n",
    "                                                                                                        information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0f62",
   "metadata": {},
   "source": [
    "### First improvement : categorical features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aea7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import encode_and_bind\n",
    "\n",
    "cat_features = ['destination_station_name', 'origin_station_name']\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_train_modified = encode_and_bind(df_train_modified, feature)\n",
    "    df_test_modified = encode_and_bind(df_test_modified, feature)\n",
    "\n",
    "df_train, df_test = df_train.align(df_test, join='left', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80645c49",
   "metadata": {},
   "source": [
    "### Second improvement : add of a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "798732c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_modified['od_origin_day'] = pd.to_datetime(df_train_modified.departure_date).dt.day\n",
    "df_test_modified['od_origin_day'] = pd.to_datetime(df_test_modified.departure_date).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc57e9",
   "metadata": {},
   "source": [
    "### Thrird improvement : convertion of cyclic features to adapted ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf6ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import cyclic_features_transform\n",
    "\n",
    "periodic_features_day = ['od_origin_day', 'sale_day'] \n",
    "periodic_features_month = ['od_origin_month', 'sale_month']\n",
    "\n",
    "df_train_modified = cyclic_features_transform(df_train_modified, periodic_features_day, periodic_features_month)\n",
    "df_test_modified = cyclic_features_transform(df_test_modified, periodic_features_day, periodic_features_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e936a",
   "metadata": {},
   "source": [
    "## Second test with transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1611a075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c5e70",
   "metadata": {},
   "source": [
    "#### Each day separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f797799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9425132792797297\n",
      "mse : 20.18575217497712\n",
      "r2 : 0.6654335337475814\n",
      "std ae : 4.051221338658282\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32a6f2",
   "metadata": {},
   "source": [
    "#### Sum of all remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cafedb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.71641791044776\n",
      "mse : 4840.327919227392\n",
      "r2 : 0.8248010957690173\n",
      "std ae : 59.70481899205154\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1068df",
   "metadata": {},
   "source": [
    "#### --> No real improvement with features engineering\n",
    "Final results with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74f4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(information_on_travel_corrected, columns = [\"sale_day_x\", \"departure_date\", \n",
    "                                                                      \"origin_station_name\", \"destination_station_name\"])\n",
    "df_results[\"prediction_until_departure\"] = prediction_until_travel\n",
    "df_results[\"real_sum_until_departure\"] = real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070b8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_classical_ml.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f195ab",
   "metadata": {},
   "source": [
    "### Third test : xgboost better? --> No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8ef244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.71641791044776\n",
      "mse : 4840.327919227392\n",
      "r2 : 0.8248010957690173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "xgtrain = xgb.DMatrix(np.array(X_train), np.array(y_train))\n",
    "xgval = xgb.DMatrix(np.array(X_val), np.array(y_val))\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xgtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(xgval, \"Validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "best_iteration = model.best_ntree_limit\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(\n",
    "    complete_preprocessed_dataset, features_names, regr, information_on_travel)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "# print(\"with split :\", mean_absolute_error(model.predict(xgtest, ntree_limit=best_iteration), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ba1c4",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da01906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc648a99",
   "metadata": {},
   "source": [
    "#### Proposition is to only consider the prices of the last 14 days as inputs for this LSTM, and the demand of the last day as target\n",
    "==> A more complex approach would require more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a76519",
   "metadata": {},
   "source": [
    "### Normalization of features : necessary for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02d5b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_demand = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_demand\"] = scaler_demand.fit_transform(np.array(df_train.demand).reshape(-1, 1))\n",
    "scaler_price = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_price\"] = scaler_price.fit_transform(np.array(df_train.price).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51179a",
   "metadata": {},
   "source": [
    "### time series separation into multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74e590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_train = splitting_travels(df_train, fewer_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83799088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9173/9173 [04:26<00:00, 34.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences\n",
    "train_window = 14\n",
    "train_inout_seq = create_inout_sequences(dataset_train, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e737dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [(torch.FloatTensor(np.array(df[0])).view(-1), torch.FloatTensor(np.array(df[1])).view(-1)) \n",
    "                   for df in train_inout_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92d5a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_class import LSTM\n",
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32ef213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(1, 100)\n",
      "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b23d8142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 0.0389371514\n",
      "epoch:   0 loss: 0.0425312482\n",
      "epoch:   0 loss: 0.0428299829\n",
      "epoch:   0 loss: 0.0421214812\n",
      "epoch:   1 loss: 0.0003015384\n",
      "epoch:   1 loss: 0.0004205655\n",
      "epoch:   1 loss: 0.0005638109\n",
      "epoch:   1 loss: 0.0007062955\n",
      "epoch:   2 loss: 0.0013477921\n",
      "epoch:   2 loss: 0.0012305866\n",
      "epoch:   2 loss: 0.0011755967\n",
      "epoch:   2 loss: 0.0013900001\n",
      "epoch:   3 loss: 0.0001851576\n",
      "epoch:   3 loss: 0.0007269164\n",
      "epoch:   3 loss: 0.0013572136\n",
      "epoch:   3 loss: 0.0019532030\n",
      "epoch:   4 loss: 0.0000858613\n",
      "epoch:   4 loss: 0.0001891613\n",
      "epoch:   4 loss: 0.0002315682\n",
      "epoch:   4 loss: 0.0002848974\n",
      "epoch:   5 loss: 0.0006630677\n",
      "epoch:   5 loss: 0.0006545676\n",
      "epoch:   5 loss: 0.0006566493\n",
      "epoch:   5 loss: 0.0006818625\n",
      "epoch:   6 loss: 0.0000198298\n",
      "epoch:   6 loss: 0.0000144879\n",
      "epoch:   6 loss: 0.0000000167\n",
      "epoch:   6 loss: 0.0000313858\n",
      "epoch:   7 loss: 0.0014428895\n",
      "epoch:   7 loss: 0.0013187550\n",
      "epoch:   7 loss: 0.0012438073\n",
      "epoch:   7 loss: 0.0012090739\n",
      "epoch:   8 loss: 0.0001067648\n",
      "epoch:   8 loss: 0.0000124952\n",
      "epoch:   8 loss: 0.0000000333\n",
      "epoch:   8 loss: 0.0000040040\n",
      "epoch:   9 loss: 0.0006378885\n",
      "epoch:   9 loss: 0.0005576430\n",
      "epoch:   9 loss: 0.0005268937\n",
      "epoch:   9 loss: 0.0004390532\n",
      "epoch:  10 loss: 0.4993573725\n",
      "epoch:  10 loss: 0.4974842072\n",
      "epoch:  10 loss: 0.4964596033\n",
      "epoch:  10 loss: 0.4960173965\n",
      "epoch:  11 loss: 0.0185449719\n",
      "epoch:  11 loss: 0.0182376001\n",
      "epoch:  11 loss: 0.0179740880\n",
      "epoch:  11 loss: 0.0179195348\n",
      "epoch:  12 loss: 0.0013165870\n",
      "epoch:  12 loss: 0.0022296938\n",
      "epoch:  12 loss: 0.0028637231\n",
      "epoch:  12 loss: 0.0033198984\n",
      "epoch:  13 loss: 0.0000245425\n",
      "epoch:  13 loss: 0.0000280103\n",
      "epoch:  13 loss: 0.0000301205\n",
      "epoch:  13 loss: 0.0000202079\n",
      "epoch:  14 loss: 0.0006310543\n",
      "epoch:  14 loss: 0.0006758834\n",
      "epoch:  14 loss: 0.0007372021\n",
      "epoch:  14 loss: 0.0008129546\n",
      "epoch:  15 loss: 0.0020698414\n",
      "epoch:  15 loss: 0.0020007149\n",
      "epoch:  15 loss: 0.0019244176\n",
      "epoch:  15 loss: 0.0018447003\n",
      "epoch:  16 loss: 0.0007728778\n",
      "epoch:  16 loss: 0.0005515119\n",
      "epoch:  16 loss: 0.0004676785\n",
      "epoch:  16 loss: 0.0004395329\n",
      "epoch:  17 loss: 0.0014029591\n",
      "epoch:  17 loss: 0.0015331192\n",
      "epoch:  17 loss: 0.0016057862\n",
      "epoch:  17 loss: 0.0016550362\n",
      "epoch:  18 loss: 0.0025677839\n",
      "epoch:  18 loss: 0.0017323398\n",
      "epoch:  18 loss: 0.0017526974\n",
      "epoch:  18 loss: 0.0016282588\n",
      "epoch:  19 loss: 0.0008102343\n",
      "epoch:  19 loss: 0.0007478469\n",
      "epoch:  19 loss: 0.0007248095\n",
      "epoch:  19 loss: 0.0007162370\n",
      "epoch:  20 loss: 0.0018440501\n",
      "epoch:  20 loss: 0.0019844053\n",
      "epoch:  20 loss: 0.0020129122\n",
      "epoch:  20 loss: 0.0020239234\n",
      "epoch:  21 loss: 0.0005473958\n",
      "epoch:  21 loss: 0.0004551392\n",
      "epoch:  21 loss: 0.0004814574\n",
      "epoch:  21 loss: 0.0005041698\n",
      "epoch:  22 loss: 0.0003321511\n",
      "epoch:  22 loss: 0.0003377100\n",
      "epoch:  22 loss: 0.0003627822\n",
      "epoch:  22 loss: 0.0003905759\n",
      "epoch:  23 loss: 0.0000842091\n",
      "epoch:  23 loss: 0.0000809303\n",
      "epoch:  23 loss: 0.0000796464\n",
      "epoch:  23 loss: 0.0000787616\n",
      "epoch:  24 loss: 0.0040030139\n",
      "epoch:  24 loss: 0.0040147281\n",
      "epoch:  24 loss: 0.0040231170\n",
      "epoch:  24 loss: 0.0040258090\n",
      "epoch:  25 loss: 0.0015638871\n",
      "epoch:  25 loss: 0.0014988978\n",
      "epoch:  25 loss: 0.0014674773\n",
      "epoch:  25 loss: 0.0014529414\n",
      "epoch:  26 loss: 0.0002020016\n",
      "epoch:  26 loss: 0.0002162546\n",
      "epoch:  26 loss: 0.0001739046\n",
      "epoch:  26 loss: 0.0001629100\n",
      "epoch:  27 loss: 0.0000000029\n",
      "epoch:  27 loss: 0.0000136139\n",
      "epoch:  27 loss: 0.0000276669\n",
      "epoch:  27 loss: 0.0000383996\n",
      "epoch:  28 loss: 0.0013477221\n",
      "epoch:  28 loss: 0.0013864957\n",
      "epoch:  28 loss: 0.0014154086\n",
      "epoch:  28 loss: 0.0014360554\n",
      "epoch:  29 loss: 0.0010679029\n",
      "epoch:  29 loss: 0.0010687951\n",
      "epoch:  29 loss: 0.0010691382\n",
      "epoch:  29 loss: 0.0010693097\n",
      "epoch:  30 loss: 0.0751230717\n",
      "epoch:  30 loss: 0.0747258961\n",
      "epoch:  30 loss: 0.0748067647\n",
      "epoch:  30 loss: 0.0756865665\n",
      "epoch:  31 loss: 0.0004052520\n",
      "epoch:  31 loss: 0.0003711515\n",
      "epoch:  31 loss: 0.0003267613\n",
      "epoch:  31 loss: 0.0003094768\n",
      "epoch:  32 loss: 0.0001130073\n",
      "epoch:  32 loss: 0.0001237649\n",
      "epoch:  32 loss: 0.0001404281\n",
      "epoch:  32 loss: 0.0001551371\n",
      "epoch:  33 loss: 0.2226491868\n",
      "epoch:  33 loss: 0.2185668349\n",
      "epoch:  33 loss: 0.2159997821\n",
      "epoch:  33 loss: 0.2137916535\n",
      "epoch:  34 loss: 0.0016607735\n",
      "epoch:  34 loss: 0.0017357949\n",
      "epoch:  34 loss: 0.0017715128\n",
      "epoch:  34 loss: 0.0017818789\n",
      "epoch:  35 loss: 0.0005908309\n",
      "epoch:  35 loss: 0.0005919586\n",
      "epoch:  35 loss: 0.0005788124\n",
      "epoch:  35 loss: 0.0005439039\n",
      "epoch:  36 loss: 0.0001952846\n",
      "epoch:  36 loss: 0.0001974864\n",
      "epoch:  36 loss: 0.0001894024\n",
      "epoch:  36 loss: 0.0001815901\n",
      "epoch:  37 loss: 0.0001512772\n",
      "epoch:  37 loss: 0.0001433519\n",
      "epoch:  37 loss: 0.0001349964\n",
      "epoch:  37 loss: 0.0001289221\n",
      "epoch:  38 loss: 0.0001347845\n",
      "epoch:  38 loss: 0.0001415379\n",
      "epoch:  38 loss: 0.0001423645\n",
      "epoch:  38 loss: 0.0001421512\n",
      "epoch:  39 loss: 0.0007554980\n",
      "epoch:  39 loss: 0.0009051232\n",
      "epoch:  39 loss: 0.0009928573\n",
      "epoch:  39 loss: 0.0010514532\n",
      "epoch:  40 loss: 0.0005939848\n",
      "epoch:  40 loss: 0.0007269935\n",
      "epoch:  40 loss: 0.0008212179\n",
      "epoch:  40 loss: 0.0008998159\n",
      "epoch:  41 loss: 0.0025760240\n",
      "epoch:  41 loss: 0.0026180386\n",
      "epoch:  41 loss: 0.0026408334\n",
      "epoch:  41 loss: 0.0026513070\n",
      "epoch:  42 loss: 0.0032069383\n",
      "epoch:  42 loss: 0.0030246533\n",
      "epoch:  42 loss: 0.0029179787\n",
      "epoch:  42 loss: 0.0028471036\n",
      "epoch:  43 loss: 0.0111141289\n",
      "epoch:  43 loss: 0.0113753909\n",
      "epoch:  43 loss: 0.0114955772\n",
      "epoch:  43 loss: 0.0115711363\n",
      "epoch:  44 loss: 0.0009320612\n",
      "epoch:  44 loss: 0.0009839712\n",
      "epoch:  44 loss: 0.0011021704\n",
      "epoch:  44 loss: 0.0012157485\n",
      "epoch:  45 loss: 0.0001098051\n",
      "epoch:  45 loss: 0.0001225993\n",
      "epoch:  45 loss: 0.0001390654\n",
      "epoch:  45 loss: 0.0001551505\n",
      "epoch:  46 loss: 0.0020047908\n",
      "epoch:  46 loss: 0.0016039571\n",
      "epoch:  46 loss: 0.0013533691\n",
      "epoch:  46 loss: 0.0011896046\n",
      "epoch:  47 loss: 0.0047596106\n",
      "epoch:  47 loss: 0.0047131507\n",
      "epoch:  47 loss: 0.0046977936\n",
      "epoch:  47 loss: 0.0047409027\n",
      "epoch:  48 loss: 0.0000164785\n",
      "epoch:  48 loss: 0.0000072728\n",
      "epoch:  48 loss: 0.0000030857\n",
      "epoch:  48 loss: 0.0000009720\n",
      "epoch:  49 loss: 0.0006330173\n",
      "epoch:  49 loss: 0.0001631078\n",
      "epoch:  49 loss: 0.0001481688\n",
      "epoch:  49 loss: 0.0001815740\n",
      "epoch:  50 loss: 0.0000022838\n",
      "epoch:  50 loss: 0.0000036687\n",
      "epoch:  50 loss: 0.0000057977\n",
      "epoch:  50 loss: 0.0000090463\n",
      "epoch:  51 loss: 0.0000359254\n",
      "epoch:  51 loss: 0.0000244004\n",
      "epoch:  51 loss: 0.0000246685\n",
      "epoch:  51 loss: 0.0000282069\n",
      "epoch:  52 loss: 0.0079667270\n",
      "epoch:  52 loss: 0.0085193664\n",
      "epoch:  52 loss: 0.0086889565\n",
      "epoch:  52 loss: 0.0086627072\n",
      "epoch:  53 loss: 0.0007027453\n",
      "epoch:  53 loss: 0.0006210859\n",
      "epoch:  53 loss: 0.0005715677\n",
      "epoch:  53 loss: 0.0005363185\n",
      "epoch:  54 loss: 0.0001631672\n",
      "epoch:  54 loss: 0.0001953846\n",
      "epoch:  54 loss: 0.0002198863\n",
      "epoch:  54 loss: 0.0002359094\n",
      "epoch:  55 loss: 0.0000070767\n",
      "epoch:  55 loss: 0.0000167301\n",
      "epoch:  55 loss: 0.0000261014\n",
      "epoch:  55 loss: 0.0000334875\n",
      "epoch:  56 loss: 0.0013396555\n",
      "epoch:  56 loss: 0.0012286763\n",
      "epoch:  56 loss: 0.0011494944\n",
      "epoch:  56 loss: 0.0010969328\n",
      "epoch:  57 loss: 0.0032282455\n",
      "epoch:  57 loss: 0.0033649851\n",
      "epoch:  57 loss: 0.0031920432\n",
      "epoch:  57 loss: 0.0033355223\n",
      "epoch:  58 loss: 0.0007889611\n",
      "epoch:  58 loss: 0.0008524635\n",
      "epoch:  58 loss: 0.0009067235\n",
      "epoch:  58 loss: 0.0009541987\n",
      "epoch:  59 loss: 0.0002013346\n",
      "epoch:  59 loss: 0.0002377458\n",
      "epoch:  59 loss: 0.0002642160\n",
      "epoch:  59 loss: 0.0002801526\n",
      "epoch:  60 loss: 0.0020400658\n",
      "epoch:  60 loss: 0.0020150039\n",
      "epoch:  60 loss: 0.0019781387\n",
      "epoch:  60 loss: 0.0019435579\n",
      "epoch:  61 loss: 0.0000023506\n",
      "epoch:  61 loss: 0.0000000144\n",
      "epoch:  61 loss: 0.0000001122\n",
      "epoch:  61 loss: 0.0000002159\n",
      "epoch:  62 loss: 0.0000543203\n",
      "epoch:  62 loss: 0.0000687833\n",
      "epoch:  62 loss: 0.0000186492\n",
      "epoch:  62 loss: 0.0000929729\n",
      "epoch:  63 loss: 0.0015859185\n",
      "epoch:  63 loss: 0.0018590591\n",
      "epoch:  63 loss: 0.0019151516\n",
      "epoch:  63 loss: 0.0019912031\n",
      "epoch:  64 loss: 0.0005608738\n",
      "epoch:  64 loss: 0.0006738983\n",
      "epoch:  64 loss: 0.0007383321\n",
      "epoch:  64 loss: 0.0007845039\n",
      "epoch:  65 loss: 0.0000006241\n",
      "epoch:  65 loss: 0.0000069993\n",
      "epoch:  65 loss: 0.0000243957\n",
      "epoch:  65 loss: 0.0000227158\n",
      "epoch:  66 loss: 0.0047670403\n",
      "epoch:  66 loss: 0.0044362461\n",
      "epoch:  66 loss: 0.0042929626\n",
      "epoch:  66 loss: 0.0042059906\n",
      "epoch:  67 loss: 0.0001233753\n",
      "epoch:  67 loss: 0.0001070174\n",
      "epoch:  67 loss: 0.0001004641\n",
      "epoch:  67 loss: 0.0000973450\n",
      "epoch:  68 loss: 0.0053770593\n",
      "epoch:  68 loss: 0.0047102952\n",
      "epoch:  68 loss: 0.0035126461\n",
      "epoch:  68 loss: 0.0032298916\n",
      "epoch:  69 loss: 0.0014773211\n",
      "epoch:  69 loss: 0.0014770783\n",
      "epoch:  69 loss: 0.0014691493\n",
      "epoch:  69 loss: 0.0014742529\n",
      "epoch:  70 loss: 0.0029656372\n",
      "epoch:  70 loss: 0.0026364734\n",
      "epoch:  70 loss: 0.0022966624\n",
      "epoch:  70 loss: 0.0019759920\n",
      "epoch:  71 loss: 0.0000032725\n",
      "epoch:  71 loss: 0.0000036262\n",
      "epoch:  71 loss: 0.0000042149\n",
      "epoch:  71 loss: 0.0000048448\n",
      "epoch:  72 loss: 0.0009529950\n",
      "epoch:  72 loss: 0.0010188342\n",
      "epoch:  72 loss: 0.0010439212\n",
      "epoch:  72 loss: 0.0010271117\n",
      "epoch:  73 loss: 0.0004008962\n",
      "epoch:  73 loss: 0.0004132318\n",
      "epoch:  73 loss: 0.0003517820\n",
      "epoch:  73 loss: 0.0003394428\n",
      "epoch:  74 loss: 0.0000009374\n",
      "epoch:  74 loss: 0.0000210061\n",
      "epoch:  74 loss: 0.0000582377\n",
      "epoch:  74 loss: 0.0000840331\n",
      "epoch:  75 loss: 0.0042673037\n",
      "epoch:  75 loss: 0.0036778864\n",
      "epoch:  75 loss: 0.0035848101\n",
      "epoch:  75 loss: 0.0035773767\n",
      "epoch:  76 loss: 0.0002256491\n",
      "epoch:  76 loss: 0.0003911038\n",
      "epoch:  76 loss: 0.0005174034\n",
      "epoch:  76 loss: 0.0006279975\n",
      "epoch:  77 loss: 0.0003134995\n",
      "epoch:  77 loss: 0.0003291922\n",
      "epoch:  77 loss: 0.0003325315\n",
      "epoch:  77 loss: 0.0003313543\n",
      "epoch:  78 loss: 0.0201227404\n",
      "epoch:  78 loss: 0.0200467650\n",
      "epoch:  78 loss: 0.0200062618\n",
      "epoch:  78 loss: 0.0199639276\n",
      "epoch:  79 loss: 0.0001436732\n",
      "epoch:  79 loss: 0.0001383985\n",
      "epoch:  79 loss: 0.0001362444\n",
      "epoch:  79 loss: 0.0001345341\n",
      "epoch:  80 loss: 0.0000231296\n",
      "epoch:  80 loss: 0.0000254538\n",
      "epoch:  80 loss: 0.0000276405\n",
      "epoch:  80 loss: 0.0000298802\n",
      "epoch:  81 loss: 0.0029223270\n",
      "epoch:  81 loss: 0.0030173869\n",
      "epoch:  81 loss: 0.0031222424\n",
      "epoch:  81 loss: 0.0030430909\n",
      "epoch:  82 loss: 0.0007838897\n",
      "epoch:  82 loss: 0.0007279516\n",
      "epoch:  82 loss: 0.0007092577\n",
      "epoch:  82 loss: 0.0007136328\n",
      "epoch:  83 loss: 0.0015149972\n",
      "epoch:  83 loss: 0.0014106138\n",
      "epoch:  83 loss: 0.0013856258\n",
      "epoch:  83 loss: 0.0014114556\n",
      "epoch:  84 loss: 0.0043535987\n",
      "epoch:  84 loss: 0.0041286820\n",
      "epoch:  84 loss: 0.0040860814\n",
      "epoch:  84 loss: 0.0040758615\n",
      "epoch:  85 loss: 0.0000074039\n",
      "epoch:  85 loss: 0.0000080604\n",
      "epoch:  85 loss: 0.0000065150\n",
      "epoch:  85 loss: 0.0000049188\n",
      "epoch:  86 loss: 0.0002536072\n",
      "epoch:  86 loss: 0.0003679479\n",
      "epoch:  86 loss: 0.0003451175\n",
      "epoch:  86 loss: 0.0004207904\n",
      "epoch:  87 loss: 0.0000178543\n",
      "epoch:  87 loss: 0.0000208294\n",
      "epoch:  87 loss: 0.0000190732\n",
      "epoch:  87 loss: 0.0000197836\n",
      "epoch:  88 loss: 0.0008529508\n",
      "epoch:  88 loss: 0.0009427260\n",
      "epoch:  88 loss: 0.0010011307\n",
      "epoch:  88 loss: 0.0010807305\n",
      "epoch:  89 loss: 0.0000404912\n",
      "epoch:  89 loss: 0.0000466331\n",
      "epoch:  89 loss: 0.0000456863\n",
      "epoch:  89 loss: 0.0000434989\n",
      "epoch:  90 loss: 0.0536693148\n",
      "epoch:  90 loss: 0.0513985157\n",
      "epoch:  90 loss: 0.0510978550\n",
      "epoch:  90 loss: 0.0510557480\n",
      "epoch:  91 loss: 0.0008349773\n",
      "epoch:  91 loss: 0.0007459346\n",
      "epoch:  91 loss: 0.0007435662\n",
      "epoch:  91 loss: 0.0007504931\n",
      "epoch:  92 loss: 0.0000567382\n",
      "epoch:  92 loss: 0.0000660245\n",
      "epoch:  92 loss: 0.0000765222\n",
      "epoch:  92 loss: 0.0000849447\n",
      "epoch:  93 loss: 0.0006616840\n",
      "epoch:  93 loss: 0.0007192199\n",
      "epoch:  93 loss: 0.0007003741\n",
      "epoch:  93 loss: 0.0006797878\n",
      "epoch:  94 loss: 0.0000007381\n",
      "epoch:  94 loss: 0.0000000199\n",
      "epoch:  94 loss: 0.0000000000\n",
      "epoch:  94 loss: 0.0000000009\n",
      "epoch:  95 loss: 0.0000487279\n",
      "epoch:  95 loss: 0.0000155912\n",
      "epoch:  95 loss: 0.0000070821\n",
      "epoch:  95 loss: 0.0000032837\n",
      "epoch:  96 loss: 0.0489382967\n",
      "epoch:  96 loss: 0.0502957180\n",
      "epoch:  96 loss: 0.0514116511\n",
      "epoch:  96 loss: 0.0513348058\n",
      "epoch:  97 loss: 0.0135578988\n",
      "epoch:  97 loss: 0.0132528329\n",
      "epoch:  97 loss: 0.0130095165\n",
      "epoch:  97 loss: 0.0128651941\n",
      "epoch:  98 loss: 0.0000056801\n",
      "epoch:  98 loss: 0.0000236705\n",
      "epoch:  98 loss: 0.0000179683\n",
      "epoch:  98 loss: 0.0000202921\n",
      "epoch:  99 loss: 0.0027991054\n",
      "epoch:  99 loss: 0.0028480196\n",
      "epoch:  99 loss: 0.0028754091\n",
      "epoch:  99 loss: 0.0028979478\n",
      "epoch:  99 loss: 0.002897947794\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch = random.choices(list_of_tensors, k=batch_size)\n",
    "    for k in range(4):\n",
    "    \n",
    "        for seq, labels in batch:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')        \n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.12f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1888309",
   "metadata": {},
   "source": [
    "#### Adaptation of the validation dataset to LSTM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ba675ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5004/5004 [01:11<00:00, 70.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences_validation\n",
    "all_validation_data = []; all_validation_target = []\n",
    "for ts in tqdm(complete_preprocessed_dataset):\n",
    "    available_part, unavailable_part = ts\n",
    "    if len(available_part) > train_window: #prediction possible\n",
    "        limit = len(available_part)\n",
    "        whole_df = pd.concat([available_part.iloc[len(available_part)-train_window:], unavailable_part])\n",
    "        features_val, target_val = create_inout_sequences_validation(whole_df, train_window, scaler_price)\n",
    "        all_validation_data.extend(features_val)\n",
    "        all_validation_target.extend(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fdbd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors_test = [(torch.FloatTensor(np.array(df)).view(-1)) for df in all_validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a90ac",
   "metadata": {},
   "source": [
    "### Predictions on validation data --> test on single demand's day each time (no sum until departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e3e8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 51310/51310 [01:45<00:00, 486.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_inputs = []\n",
    "kept_targets = []\n",
    "for i in tqdm(range(len(list_of_tensors_test))):\n",
    "    seq = torch.FloatTensor(list_of_tensors_test[i])\n",
    "    if len(seq)>0:\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            kept_targets.append(all_validation_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b7b37",
   "metadata": {},
   "source": [
    "#### To inverse previous scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8610fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51310, 1)\n",
      "(51310,)\n"
     ]
    }
   ],
   "source": [
    "actual_predictions = scaler_demand.inverse_transform(np.array(test_inputs).reshape(-1, 1))\n",
    "print(actual_predictions.shape)\n",
    "print(np.array(kept_targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63d966",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8930535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 7.824755431550494\n",
      "mse : 201.60545022256156\n",
      "r2 : -0.0438340376861186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(kept_targets, actual_predictions))\n",
    "print(\"mse :\", mean_squared_error(kept_targets, actual_predictions))\n",
    "print(\"r2 :\", r2_score(kept_targets, actual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9793197",
   "metadata": {},
   "source": [
    "### Conclusion on deep learning approach : \n",
    "- does not work well, no convergence (seen during the training), then performances remain bad with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f9ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
