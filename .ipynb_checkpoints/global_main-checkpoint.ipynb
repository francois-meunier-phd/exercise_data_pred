{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65b909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8029d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_loading import file_loader, data_types_converter\n",
    "\n",
    "file_path = \"../dataset/\"\n",
    "\n",
    "df_train = file_loader(file_path + \"training/leaf.csv.lz4\")\n",
    "df_test = file_loader(file_path + \"testing/leaf.csv.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7543346",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = ['demand', 'destination_current_public_holiday', 'destination_current_school_holiday', \n",
    "                  'destination_days_to_next_public_holiday', 'destination_days_to_next_school_holiday', \n",
    "                  'od_destination_time', 'od_number_of_similar_12_hours', \n",
    "                  'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours', 'od_origin_month', \n",
    "                  'od_origin_time', 'od_origin_week', 'od_origin_weekday', 'od_origin_year', \n",
    "                  'od_travel_time_minutes', 'origin_current_public_holiday', 'origin_current_school_holiday', \n",
    "                  'origin_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', \n",
    "                  'price', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week', 'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81bcc8",
   "metadata": {},
   "source": [
    "#### Convertion of data types to float when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c9ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_types_converter(df_train, numerical_data)\n",
    "df_test = data_types_converter(df_test, numerical_data)\n",
    "\n",
    "df_train_modified = df_train.copy()\n",
    "df_test_modified = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae25c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (632841, 31)\n",
      "test : (32565, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"train :\", df_train_modified.shape)\n",
    "print(\"test :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfbf88",
   "metadata": {},
   "source": [
    "# Creation of dataset to predict on : \n",
    "- evaluation is on the sum of all remaining days until departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38f4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_be_used = [-90, -60, -30, -20, -15, -10, -7, -6, -5, -3, -2, -1]\n",
    "static_features = ['departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday', 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'origin_station_name']#13911 elements if groupby on these\n",
    "fewer_static_features = ['destination_station_name', 'origin_station_name', 'departure_date', \n",
    "                         'od_origin_time', 'od_destination_time']#9173 elements if groupby on these\n",
    "#Differences in terms of size of groupby results would need further data exploration \n",
    "\n",
    "changing_features = ['demand', 'price',\n",
    "       'sale_date', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62871ea",
   "metadata": {},
   "source": [
    "#### groupby realized to create time series based of same travels:\n",
    "- number of results changes when more (apparently static) features are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbe9b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [00:22<00:00, 18.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_test = splitting_travels(df_test, fewer_static_features)\n",
    "# print(len(dataset_test), \"different travels in the dataset\")\n",
    "complete_preprocessed_dataset, information_on_travel = extraction_validation_set(dataset_test, days_to_be_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59954e6d",
   "metadata": {},
   "source": [
    "## First test for prediction --> baseline with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3681c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's only consider numerical features\n",
    "features_names = [#'departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',# 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', #'origin_station_name', #'sale_date', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae513e4",
   "metadata": {},
   "source": [
    "#### Prediction of each day separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7073e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9422947216326971\n",
      "mse : 20.178494562962864\n",
      "r2 : 0.6655538242170171\n",
      "std ae : 4.050430320012616\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd1480",
   "metadata": {},
   "source": [
    "#### Prediction on the sum of remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3be324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.682396839332746\n",
      "mse : 4830.855355575066\n",
      "r2 : 0.8251439615417171\n",
      "std ae : 59.64580380358238\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, \n",
    "                                                                                                        information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e46f2",
   "metadata": {},
   "source": [
    "### First improvement : categorical features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ab2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import encode_and_bind\n",
    "\n",
    "cat_features = ['destination_station_name', 'origin_station_name']\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_train_modified = encode_and_bind(df_train_modified, feature)\n",
    "    df_test_modified = encode_and_bind(df_test_modified, feature)\n",
    "\n",
    "df_train, df_test = df_train.align(df_test, join='left', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d8909",
   "metadata": {},
   "source": [
    "### Second improvement : add of a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6beeebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_modified['od_origin_day'] = pd.to_datetime(df_train_modified.departure_date).dt.day\n",
    "df_test_modified['od_origin_day'] = pd.to_datetime(df_test_modified.departure_date).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673677f6",
   "metadata": {},
   "source": [
    "### Thrird improvement : convertion of cyclic features to adapted ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6716b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import cyclic_features_transform\n",
    "\n",
    "periodic_features_day = ['od_origin_day', 'sale_day'] \n",
    "periodic_features_month = ['od_origin_month', 'sale_month']\n",
    "\n",
    "df_train_modified = cyclic_features_transform(df_train_modified, periodic_features_day, periodic_features_month)\n",
    "df_test_modified = cyclic_features_transform(df_test_modified, periodic_features_day, periodic_features_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fd926",
   "metadata": {},
   "source": [
    "## Second test with transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa60dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56d90c",
   "metadata": {},
   "source": [
    "#### Each day separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41627988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9425132792797297\n",
      "mse : 20.18575217497712\n",
      "r2 : 0.6654335337475814\n",
      "std ae : 4.051221338658282\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb546c",
   "metadata": {},
   "source": [
    "#### Sum of all remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adba50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.71641791044776\n",
      "mse : 4840.327919227392\n",
      "r2 : 0.8248010957690173\n",
      "std ae : 59.70481899205154\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std ae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410ad07",
   "metadata": {},
   "source": [
    "#### --> No real improvement with features engineering\n",
    "Final results with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b45f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(information_on_travel_corrected, columns = [\"sale_day_x\", \"departure_date\", \n",
    "                                                                      \"origin_station_name\", \"destination_station_name\"])\n",
    "df_results[\"prediction_until_departure\"] = prediction_until_travel\n",
    "df_results[\"real_sum_until_departure\"] = real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f12d1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results_classical_ml.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fcce2",
   "metadata": {},
   "source": [
    "### Third test : xgboost better? --> No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "xgtrain = xgb.DMatrix(np.array(X_train), np.array(y_train))\n",
    "xgval = xgb.DMatrix(np.array(X_val), np.array(y_val))\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xgtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(xgval, \"Validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "best_iteration = model.best_ntree_limit\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(\n",
    "    complete_preprocessed_dataset, features_names, regr, information_on_travel)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "# print(\"with split :\", mean_absolute_error(model.predict(xgtest, ntree_limit=best_iteration), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126de16b",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97846a8",
   "metadata": {},
   "source": [
    "#### Proposition is to only consider the prices of the last 14 days as inputs for this LSTM, and the demand of the last day as target\n",
    "==> A more complex approach would require more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919f579",
   "metadata": {},
   "source": [
    "### Normalization of features : necessary for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbea763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_demand = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_demand\"] = scaler_demand.fit_transform(np.array(df_train.demand).reshape(-1, 1))\n",
    "scaler_price = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_price\"] = scaler_price.fit_transform(np.array(df_train.price).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16ded7",
   "metadata": {},
   "source": [
    "### time series separation into multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38336b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_train = splitting_travels(df_train, fewer_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import create_inout_sequences\n",
    "train_window = 14\n",
    "train_inout_seq = create_inout_sequences(dataset_train, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f478ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [(torch.FloatTensor(np.array(df[0])).view(-1), torch.FloatTensor(np.array(df[1])).view(-1)) \n",
    "                   for df in train_inout_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_class import LSTM\n",
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch = random.choices(list_of_tensors, k=batch_size)\n",
    "    for k in range(4):\n",
    "    \n",
    "        for seq, labels in batch:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')        \n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.12f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de15840",
   "metadata": {},
   "source": [
    "#### Adaptation of the validation dataset to LSTM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import create_inout_sequences_validation\n",
    "all_validation_data = []; all_validation_target = []\n",
    "for ts in tqdm(complete_preprocessed_dataset):\n",
    "    available_part, unavailable_part = ts\n",
    "    if len(available_part) > train_window: #prediction possible\n",
    "        limit = len(available_part)\n",
    "        whole_df = pd.concat([available_part.iloc[len(available_part)-train_window:], unavailable_part])\n",
    "        features_val, target_val = create_inout_sequences_validation(whole_df, train_window, scaler_price)\n",
    "        all_validation_data.extend(features_val)\n",
    "        all_validation_target.extend(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors_test = [(torch.FloatTensor(np.array(df)).view(-1)) for df in all_validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c31b2",
   "metadata": {},
   "source": [
    "### Predictions on validation data --> test on single demand's day each time (no sum until departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672eb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_inputs = []\n",
    "kept_targets = []\n",
    "for i in tqdm(range(len(list_of_tensors_test))):\n",
    "    seq = torch.FloatTensor(list_of_tensors_test[i])\n",
    "    if len(seq)>0:\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            kept_targets.append(all_validation_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6ed0d",
   "metadata": {},
   "source": [
    "#### To inverse previous scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffda6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = scaler_demand.inverse_transform(np.array(test_inputs).reshape(-1, 1))\n",
    "print(actual_predictions.shape)\n",
    "print(np.array(kept_targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633de365",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(kept_targets, actual_predictions))\n",
    "print(\"mse :\", mean_squared_error(kept_targets, actual_predictions))\n",
    "print(\"r2 :\", r2_score(kept_targets, actual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2d7e7",
   "metadata": {},
   "source": [
    "### Conclusion on deep learning approach : \n",
    "- does not work well, no convergence (seen during the training), then performances remain bad with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
