{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "903788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1e4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_loading import file_loader, data_types_converter\n",
    "\n",
    "file_path = \"../dataset/\"\n",
    "\n",
    "df_train = file_loader(file_path + \"training/leaf.csv.lz4\")\n",
    "df_test = file_loader(file_path + \"testing/leaf.csv.lz4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf35068",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = ['demand', 'destination_current_public_holiday', 'destination_current_school_holiday', \n",
    "                  'destination_days_to_next_public_holiday', 'destination_days_to_next_school_holiday', \n",
    "                  'od_destination_time', 'od_number_of_similar_12_hours', \n",
    "                  'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours', 'od_origin_month', \n",
    "                  'od_origin_time', 'od_origin_week', 'od_origin_weekday', 'od_origin_year', \n",
    "                  'od_travel_time_minutes', 'origin_current_public_holiday', 'origin_current_school_holiday', \n",
    "                  'origin_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', \n",
    "                  'price', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week', 'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af589769",
   "metadata": {},
   "source": [
    "#### Convertion of data types to float when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f8848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_types_converter(df_train, numerical_data)\n",
    "df_test = data_types_converter(df_test, numerical_data)\n",
    "\n",
    "df_train_modified = df_train.copy()\n",
    "df_test_modified = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c45549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (632841, 31)\n",
      "test : (32565, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"train :\", df_train_modified.shape)\n",
    "print(\"test :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42adffa",
   "metadata": {},
   "source": [
    "# Creation of dataset to predict on : \n",
    "- evaluation is on the sum of all remaining days until departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ab3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_be_used = [-90, -60, -30, -20, -15, -10, -7, -6, -5, -3, -2, -1]\n",
    "static_features = ['departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday', 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'origin_station_name']#13911 elements if groupby on these\n",
    "fewer_static_features = ['destination_station_name', 'origin_station_name', 'departure_date', \n",
    "                         'od_origin_time', 'od_destination_time']#9173 elements if groupby on these\n",
    "#Differences in terms of size of groupby results would need further data exploration \n",
    "\n",
    "changing_features = ['demand', 'price',\n",
    "       'sale_date', 'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29644238",
   "metadata": {},
   "source": [
    "#### groupby realized to create time series based of same travels:\n",
    "- number of results changes when more (apparently static) features are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67003e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [00:07<00:00, 55.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_test = splitting_travels(df_test, fewer_static_features)\n",
    "# print(len(dataset_test), \"different travels in the dataset\")\n",
    "complete_preprocessed_dataset, information_on_travel = extraction_validation_set(dataset_test, days_to_be_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ac9c7",
   "metadata": {},
   "source": [
    "## First test for prediction --> baseline with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1179d808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [#'departure_date',\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',# 'destination_station_name',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', #'origin_station_name', #'sale_date', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06792c27",
   "metadata": {},
   "source": [
    "#### Prediction of each day separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5295c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.9448514836266901\n",
      "mse : 20.22706059878564\n",
      "r2 : 0.6647488719494837\n",
      "std mae : 4.05519584057524\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649b4af",
   "metadata": {},
   "source": [
    "#### Prediction on the sum of remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d5f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.795654082528536\n",
      "mse : 4872.257023705004\n",
      "r2 : 0.8236454004915686\n",
      "std mae : 59.92435375128348\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, \n",
    "                                                                                                        information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891afbd6",
   "metadata": {},
   "source": [
    "### First improvement : categorical features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05755f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import encode_and_bind\n",
    "\n",
    "cat_features = ['destination_station_name', 'origin_station_name']\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_train_modified = encode_and_bind(df_train_modified, feature)\n",
    "    df_test_modified = encode_and_bind(df_test_modified, feature)\n",
    "\n",
    "df_train, df_test = df_train.align(df_test, join='left', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e188f3",
   "metadata": {},
   "source": [
    "### Second improvement : add of a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfedd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_modified['od_origin_day'] = pd.to_datetime(df_train_modified.departure_date).dt.day\n",
    "df_test_modified['od_origin_day'] = pd.to_datetime(df_test_modified.departure_date).dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc2ca9",
   "metadata": {},
   "source": [
    "### Thrird improvement : convertion of cyclic features to adapted ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9972bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import cyclic_features_transform\n",
    "\n",
    "periodic_features_day = ['od_origin_day', 'sale_day'] \n",
    "periodic_features_month = ['od_origin_month', 'sale_month']\n",
    "\n",
    "df_train_modified = cyclic_features_transform(df_train_modified, periodic_features_day, periodic_features_month)\n",
    "df_test_modified = cyclic_features_transform(df_test_modified, periodic_features_day, periodic_features_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636989e",
   "metadata": {},
   "source": [
    "## Second test with transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc044fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names = [\n",
    "       'destination_current_public_holiday',\n",
    "       'destination_current_school_holiday',\n",
    "       'destination_days_to_next_public_holiday',\n",
    "       'destination_days_to_next_school_holiday',\n",
    "       'od_destination_time', 'od_number_of_similar_12_hours',\n",
    "       'od_number_of_similar_2_hours', 'od_number_of_similar_4_hours',\n",
    "       'od_origin_month', 'od_origin_time', 'od_origin_week',\n",
    "       'od_origin_weekday', 'od_origin_year', 'od_travel_time_minutes',\n",
    "       'origin_current_public_holiday', 'origin_current_school_holiday',\n",
    "       'origin_days_to_next_public_holiday',\n",
    "       'origin_days_to_next_school_holiday', 'price', \n",
    "       'sale_day', 'sale_day_x', 'sale_month', 'sale_week',\n",
    "       'sale_weekday', 'sale_year']\n",
    "\n",
    "target_train = df_train_modified.demand\n",
    "features_train = df_train_modified[features_names]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=100, max_depth=6)#, random_state=27)\n",
    "regr.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd281eab",
   "metadata": {},
   "source": [
    "#### Each day separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9384ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.943964319688165\n",
      "mse : 20.210864759835435\n",
      "r2 : 0.6650173080502836\n",
      "std mae : 4.053623993862367\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values = df_test_modified.demand\n",
    "features_test = df_test_modified[features_names]\n",
    "prediction_all_days = regr.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_all_days))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_all_days))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_all_days))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(real_values - prediction_all_days)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9c075",
   "metadata": {},
   "source": [
    "#### Sum of all remaining days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b19843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.754389815627746\n",
      "mse : 4853.994512730465\n",
      "r2 : 0.8243064242826502\n",
      "std mae : 59.796472484943465\n"
     ]
    }
   ],
   "source": [
    "from classical_ml_evaluation import protocol_classical_ML_predictor\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(complete_preprocessed_dataset, \n",
    "                                                                                                        features_names, regr, information_on_travel)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "print(\"std mae :\", np.std(np.abs(np.array(real_values) - np.array(prediction_until_travel))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70d584",
   "metadata": {},
   "source": [
    "#### --> No real improvement with features engineering\n",
    "Final results with classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c0454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(information_on_travel_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c83fb",
   "metadata": {},
   "source": [
    "### Third test : xgboost better? --> No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586ed309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 35.754389815627746\n",
      "mse : 4853.994512730465\n",
      "r2 : 0.8243064242826502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':0.01,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "xgtrain = xgb.DMatrix(np.array(X_train), np.array(y_train))\n",
    "xgval = xgb.DMatrix(np.array(X_val), np.array(y_val))\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    xgtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(xgval, \"Validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "best_iteration = model.best_ntree_limit\n",
    "\n",
    "real_values, prediction_until_travel, information_on_travel_corrected = protocol_classical_ML_predictor(\n",
    "    complete_preprocessed_dataset, features_names, regr, information_on_travel)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(real_values, prediction_until_travel))\n",
    "print(\"mse :\", mean_squared_error(real_values, prediction_until_travel))\n",
    "print(\"r2 :\", r2_score(real_values, prediction_until_travel))\n",
    "\n",
    "# print(\"with split :\", mean_absolute_error(model.predict(xgtest, ntree_limit=best_iteration), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79573c2",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e0091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ce017",
   "metadata": {},
   "source": [
    "#### Proposition is to only consider the prices of the last 14 days as inputs for this LSTM, and the demand of the last day as target\n",
    "==> A more complex approach would require more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade57a5",
   "metadata": {},
   "source": [
    "### Normalization of features : necessary for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a5e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_demand = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_demand\"] = scaler_demand.fit_transform(np.array(df_train.demand).reshape(-1, 1))\n",
    "scaler_price = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_train[\"scaled_price\"] = scaler_price.fit_transform(np.array(df_train.price).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94712c95",
   "metadata": {},
   "source": [
    "### time series separation into multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5701f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pretreatment import splitting_travels, extraction_validation_set\n",
    "\n",
    "dataset_train = splitting_travels(df_train, fewer_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cebb77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aefad8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9173/9173 [02:23<00:00, 63.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences\n",
    "train_inout_seq = create_inout_sequences(dataset_train, train_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2809133",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [(torch.FloatTensor(np.array(df[0])).view(-1), torch.FloatTensor(np.array(df[1])).view(-1)) for df in train_inout_seq]\n",
    "# X_train_data_normalized = torch.stack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "680086d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac62f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57239f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(1, 100)\n",
      "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc573257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 0.0105583230\n",
      "epoch:   0 loss: 0.0017262325\n",
      "epoch:   0 loss: 0.0002158341\n",
      "epoch:   0 loss: 0.0000237819\n",
      "epoch:   1 loss: 0.0000511011\n",
      "epoch:   1 loss: 0.0000822764\n",
      "epoch:   1 loss: 0.0000959317\n",
      "epoch:   1 loss: 0.0000998092\n",
      "epoch:   2 loss: 0.0028644504\n",
      "epoch:   2 loss: 0.0000458622\n",
      "epoch:   2 loss: 0.0000156864\n",
      "epoch:   2 loss: 0.0000591335\n",
      "epoch:   3 loss: 0.0001752766\n",
      "epoch:   3 loss: 0.0001524745\n",
      "epoch:   3 loss: 0.0001261382\n",
      "epoch:   3 loss: 0.0001098689\n",
      "epoch:   4 loss: 0.0056758602\n",
      "epoch:   4 loss: 0.0058970940\n",
      "epoch:   4 loss: 0.0060749669\n",
      "epoch:   4 loss: 0.0062074335\n",
      "epoch:   5 loss: 0.0096435258\n",
      "epoch:   5 loss: 0.0121564679\n",
      "epoch:   5 loss: 0.0123292319\n",
      "epoch:   5 loss: 0.0122191645\n",
      "epoch:   6 loss: 0.0046333782\n",
      "epoch:   6 loss: 0.0052639088\n",
      "epoch:   6 loss: 0.0055931341\n",
      "epoch:   6 loss: 0.0058165742\n",
      "epoch:   7 loss: 0.0012834786\n",
      "epoch:   7 loss: 0.0012229083\n",
      "epoch:   7 loss: 0.0011629033\n",
      "epoch:   7 loss: 0.0010840759\n",
      "epoch:   8 loss: 0.0006526049\n",
      "epoch:   8 loss: 0.0006044226\n",
      "epoch:   8 loss: 0.0005753159\n",
      "epoch:   8 loss: 0.0005569592\n",
      "epoch:   9 loss: 0.0001882198\n",
      "epoch:   9 loss: 0.0001142233\n",
      "epoch:   9 loss: 0.0000867873\n",
      "epoch:   9 loss: 0.0000696521\n",
      "epoch:  10 loss: 0.0010683665\n",
      "epoch:  10 loss: 0.0012081374\n",
      "epoch:  10 loss: 0.0013140362\n",
      "epoch:  10 loss: 0.0013969465\n",
      "epoch:  11 loss: 0.0017695014\n",
      "epoch:  11 loss: 0.0017617473\n",
      "epoch:  11 loss: 0.0017416455\n",
      "epoch:  11 loss: 0.0017125353\n",
      "epoch:  12 loss: 0.0231377371\n",
      "epoch:  12 loss: 0.0229228176\n",
      "epoch:  12 loss: 0.0226630624\n",
      "epoch:  12 loss: 0.0224360228\n",
      "epoch:  13 loss: 0.0000128769\n",
      "epoch:  13 loss: 0.0000025771\n",
      "epoch:  13 loss: 0.0000001537\n",
      "epoch:  13 loss: 0.0000000023\n",
      "epoch:  14 loss: 0.0003361957\n",
      "epoch:  14 loss: 0.0002662137\n",
      "epoch:  14 loss: 0.0002859345\n",
      "epoch:  14 loss: 0.0003063788\n",
      "epoch:  15 loss: 0.0074200546\n",
      "epoch:  15 loss: 0.0079857744\n",
      "epoch:  15 loss: 0.0082973167\n",
      "epoch:  15 loss: 0.0083518187\n",
      "epoch:  16 loss: 0.0007305302\n",
      "epoch:  16 loss: 0.0008802558\n",
      "epoch:  16 loss: 0.0008337136\n",
      "epoch:  16 loss: 0.0007735076\n",
      "epoch:  17 loss: 0.0096882377\n",
      "epoch:  17 loss: 0.0100991093\n",
      "epoch:  17 loss: 0.0100391321\n",
      "epoch:  17 loss: 0.0103953807\n",
      "epoch:  18 loss: 0.0140740369\n",
      "epoch:  18 loss: 0.0141661400\n",
      "epoch:  18 loss: 0.0141370120\n",
      "epoch:  18 loss: 0.0141993603\n",
      "epoch:  19 loss: 0.1369663477\n",
      "epoch:  19 loss: 0.1371247321\n",
      "epoch:  19 loss: 0.1369733214\n",
      "epoch:  19 loss: 0.1367417425\n",
      "epoch:  20 loss: 0.0000356011\n",
      "epoch:  20 loss: 0.0000332526\n",
      "epoch:  20 loss: 0.0000386963\n",
      "epoch:  20 loss: 0.0000437769\n",
      "epoch:  21 loss: 0.0000123947\n",
      "epoch:  21 loss: 0.0000148927\n",
      "epoch:  21 loss: 0.0000157285\n",
      "epoch:  21 loss: 0.0000159205\n",
      "epoch:  22 loss: 0.0054640886\n",
      "epoch:  22 loss: 0.0055931788\n",
      "epoch:  22 loss: 0.0056997747\n",
      "epoch:  22 loss: 0.0057873628\n",
      "epoch:  23 loss: 0.0038402581\n",
      "epoch:  23 loss: 0.0035630595\n",
      "epoch:  23 loss: 0.0033506653\n",
      "epoch:  23 loss: 0.0031851837\n",
      "epoch:  24 loss: 0.0070266193\n",
      "epoch:  24 loss: 0.0067860219\n",
      "epoch:  24 loss: 0.0066741640\n",
      "epoch:  24 loss: 0.0066887508\n",
      "epoch:  25 loss: 0.0002003801\n",
      "epoch:  25 loss: 0.0001742050\n",
      "epoch:  25 loss: 0.0001560992\n",
      "epoch:  25 loss: 0.0001485536\n",
      "epoch:  26 loss: 0.0004332326\n",
      "epoch:  26 loss: 0.0004497383\n",
      "epoch:  26 loss: 0.0004827505\n",
      "epoch:  26 loss: 0.0004995124\n",
      "epoch:  27 loss: 0.0005764373\n",
      "epoch:  27 loss: 0.0005462641\n",
      "epoch:  27 loss: 0.0005512599\n",
      "epoch:  27 loss: 0.0005651874\n",
      "epoch:  28 loss: 0.0003367555\n",
      "epoch:  28 loss: 0.0003174653\n",
      "epoch:  28 loss: 0.0003053447\n",
      "epoch:  28 loss: 0.0002984804\n",
      "epoch:  29 loss: 0.0013239767\n",
      "epoch:  29 loss: 0.0013846364\n",
      "epoch:  29 loss: 0.0014278544\n",
      "epoch:  29 loss: 0.0014599885\n",
      "epoch:  30 loss: 0.0030499143\n",
      "epoch:  30 loss: 0.0031203844\n",
      "epoch:  30 loss: 0.0031662395\n",
      "epoch:  30 loss: 0.0031976830\n",
      "epoch:  31 loss: 0.0000391612\n",
      "epoch:  31 loss: 0.0000492017\n",
      "epoch:  31 loss: 0.0000583351\n",
      "epoch:  31 loss: 0.0000672877\n",
      "epoch:  32 loss: 0.0040621739\n",
      "epoch:  32 loss: 0.0040751463\n",
      "epoch:  32 loss: 0.0041102427\n",
      "epoch:  32 loss: 0.0041376641\n",
      "epoch:  33 loss: 0.0016018667\n",
      "epoch:  33 loss: 0.0018856395\n",
      "epoch:  33 loss: 0.0020070812\n",
      "epoch:  33 loss: 0.0020813716\n",
      "epoch:  34 loss: 0.0008934691\n",
      "epoch:  34 loss: 0.0011231098\n",
      "epoch:  34 loss: 0.0011772895\n",
      "epoch:  34 loss: 0.0011817111\n",
      "epoch:  35 loss: 0.0006354942\n",
      "epoch:  35 loss: 0.0005295623\n",
      "epoch:  35 loss: 0.0004603371\n",
      "epoch:  35 loss: 0.0004133457\n",
      "epoch:  36 loss: 0.0002299113\n",
      "epoch:  36 loss: 0.0002846057\n",
      "epoch:  36 loss: 0.0003256955\n",
      "epoch:  36 loss: 0.0003562410\n",
      "epoch:  37 loss: 0.0020381494\n",
      "epoch:  37 loss: 0.0017007210\n",
      "epoch:  37 loss: 0.0015168585\n",
      "epoch:  37 loss: 0.0013960111\n",
      "epoch:  38 loss: 0.0000006734\n",
      "epoch:  38 loss: 0.0000027046\n",
      "epoch:  38 loss: 0.0000063517\n",
      "epoch:  38 loss: 0.0000091110\n",
      "epoch:  39 loss: 0.0000595670\n",
      "epoch:  39 loss: 0.0000585137\n",
      "epoch:  39 loss: 0.0000583205\n",
      "epoch:  39 loss: 0.0000583879\n",
      "epoch:  40 loss: 0.0003903191\n",
      "epoch:  40 loss: 0.0004197615\n",
      "epoch:  40 loss: 0.0004392680\n",
      "epoch:  40 loss: 0.0004532491\n",
      "epoch:  41 loss: 0.0130511299\n",
      "epoch:  41 loss: 0.0130544528\n",
      "epoch:  41 loss: 0.0131048011\n",
      "epoch:  41 loss: 0.0131558077\n",
      "epoch:  42 loss: 0.0059605390\n",
      "epoch:  42 loss: 0.0051419381\n",
      "epoch:  42 loss: 0.0047843978\n",
      "epoch:  42 loss: 0.0045997645\n",
      "epoch:  43 loss: 0.2405902594\n",
      "epoch:  43 loss: 0.2404697537\n",
      "epoch:  43 loss: 0.2403807938\n",
      "epoch:  43 loss: 0.2402957678\n",
      "epoch:  44 loss: 0.0781686381\n",
      "epoch:  44 loss: 0.0781315863\n",
      "epoch:  44 loss: 0.0780078396\n",
      "epoch:  44 loss: 0.0778694004\n",
      "epoch:  45 loss: 0.0000250996\n",
      "epoch:  45 loss: 0.0000294647\n",
      "epoch:  45 loss: 0.0000339471\n",
      "epoch:  45 loss: 0.0000387275\n",
      "epoch:  46 loss: 0.0000730972\n",
      "epoch:  46 loss: 0.0000984561\n",
      "epoch:  46 loss: 0.0001214733\n",
      "epoch:  46 loss: 0.0001410079\n",
      "epoch:  47 loss: 0.0002125941\n",
      "epoch:  47 loss: 0.0001999871\n",
      "epoch:  47 loss: 0.0001973642\n",
      "epoch:  47 loss: 0.0001981940\n",
      "epoch:  48 loss: 0.0015172067\n",
      "epoch:  48 loss: 0.0015578211\n",
      "epoch:  48 loss: 0.0015435223\n",
      "epoch:  48 loss: 0.0015245288\n",
      "epoch:  49 loss: 0.0000145601\n",
      "epoch:  49 loss: 0.0000133256\n",
      "epoch:  49 loss: 0.0000116353\n",
      "epoch:  49 loss: 0.0000101551\n",
      "epoch:  50 loss: 0.0011400805\n",
      "epoch:  50 loss: 0.0010617994\n",
      "epoch:  50 loss: 0.0010319464\n",
      "epoch:  50 loss: 0.0010185868\n",
      "epoch:  51 loss: 0.0003937061\n",
      "epoch:  51 loss: 0.0004517301\n",
      "epoch:  51 loss: 0.0004802576\n",
      "epoch:  51 loss: 0.0005018064\n",
      "epoch:  52 loss: 0.0000049190\n",
      "epoch:  52 loss: 0.0000077381\n",
      "epoch:  52 loss: 0.0000111596\n",
      "epoch:  52 loss: 0.0000143986\n",
      "epoch:  53 loss: 0.0032221796\n",
      "epoch:  53 loss: 0.0028804934\n",
      "epoch:  53 loss: 0.0026794327\n",
      "epoch:  53 loss: 0.0025442492\n",
      "epoch:  54 loss: 0.0042136246\n",
      "epoch:  54 loss: 0.0038906790\n",
      "epoch:  54 loss: 0.0037542609\n",
      "epoch:  54 loss: 0.0036868926\n",
      "epoch:  55 loss: 0.0000210389\n",
      "epoch:  55 loss: 0.0000387468\n",
      "epoch:  55 loss: 0.0000489612\n",
      "epoch:  55 loss: 0.0000541483\n",
      "epoch:  56 loss: 0.0034409293\n",
      "epoch:  56 loss: 0.0033914528\n",
      "epoch:  56 loss: 0.0033559601\n",
      "epoch:  56 loss: 0.0033272586\n",
      "epoch:  57 loss: 0.0069990666\n",
      "epoch:  57 loss: 0.0070625790\n",
      "epoch:  57 loss: 0.0069429809\n",
      "epoch:  57 loss: 0.0068216370\n",
      "epoch:  58 loss: 0.0050242189\n",
      "epoch:  58 loss: 0.0049214181\n",
      "epoch:  58 loss: 0.0048564868\n",
      "epoch:  58 loss: 0.0048067695\n",
      "epoch:  59 loss: 0.0006981076\n",
      "epoch:  59 loss: 0.0007214019\n",
      "epoch:  59 loss: 0.0007242223\n",
      "epoch:  59 loss: 0.0007207905\n",
      "epoch:  60 loss: 0.0003855692\n",
      "epoch:  60 loss: 0.0004215684\n",
      "epoch:  60 loss: 0.0004480133\n",
      "epoch:  60 loss: 0.0004704618\n",
      "epoch:  61 loss: 0.0046708123\n",
      "epoch:  61 loss: 0.0044425209\n",
      "epoch:  61 loss: 0.0043236823\n",
      "epoch:  61 loss: 0.0042485571\n",
      "epoch:  62 loss: 0.0004892811\n",
      "epoch:  62 loss: 0.0007627896\n",
      "epoch:  62 loss: 0.0009904172\n",
      "epoch:  62 loss: 0.0011737132\n",
      "epoch:  63 loss: 0.0003566326\n",
      "epoch:  63 loss: 0.0003798207\n",
      "epoch:  63 loss: 0.0003930535\n",
      "epoch:  63 loss: 0.0003142514\n",
      "epoch:  64 loss: 0.0000217346\n",
      "epoch:  64 loss: 0.0000492870\n",
      "epoch:  64 loss: 0.0000807288\n",
      "epoch:  64 loss: 0.0001073964\n",
      "epoch:  65 loss: 0.0003996560\n",
      "epoch:  65 loss: 0.0003093762\n",
      "epoch:  65 loss: 0.0002861644\n",
      "epoch:  65 loss: 0.0002826182\n",
      "epoch:  66 loss: 0.0010659520\n",
      "epoch:  66 loss: 0.0013014871\n",
      "epoch:  66 loss: 0.0014738089\n",
      "epoch:  66 loss: 0.0016021339\n",
      "epoch:  67 loss: 0.0009593389\n",
      "epoch:  67 loss: 0.0008863994\n",
      "epoch:  67 loss: 0.0008508876\n",
      "epoch:  67 loss: 0.0008264221\n",
      "epoch:  68 loss: 0.0001254269\n",
      "epoch:  68 loss: 0.0001346517\n",
      "epoch:  68 loss: 0.0001422066\n",
      "epoch:  68 loss: 0.0001481267\n",
      "epoch:  69 loss: 0.0606316216\n",
      "epoch:  69 loss: 0.0608630590\n",
      "epoch:  69 loss: 0.0609381646\n",
      "epoch:  69 loss: 0.0609620623\n",
      "epoch:  70 loss: 0.0003209177\n",
      "epoch:  70 loss: 0.0002967899\n",
      "epoch:  70 loss: 0.0002687443\n",
      "epoch:  70 loss: 0.0002417826\n",
      "epoch:  71 loss: 0.0013470701\n",
      "epoch:  71 loss: 0.0015764333\n",
      "epoch:  71 loss: 0.0017695515\n",
      "epoch:  71 loss: 0.0019532188\n",
      "epoch:  72 loss: 0.0000001449\n",
      "epoch:  72 loss: 0.0000004517\n",
      "epoch:  72 loss: 0.0000008018\n",
      "epoch:  72 loss: 0.0000011599\n",
      "epoch:  73 loss: 0.0000714736\n",
      "epoch:  73 loss: 0.0001037622\n",
      "epoch:  73 loss: 0.0001292634\n",
      "epoch:  73 loss: 0.0001500437\n",
      "epoch:  74 loss: 0.0031443429\n",
      "epoch:  74 loss: 0.0029124885\n",
      "epoch:  74 loss: 0.0026809014\n",
      "epoch:  74 loss: 0.0025366903\n",
      "epoch:  75 loss: 0.0002614813\n",
      "epoch:  75 loss: 0.0002685216\n",
      "epoch:  75 loss: 0.0002736285\n",
      "epoch:  75 loss: 0.0002730845\n",
      "epoch:  76 loss: 0.0006196103\n",
      "epoch:  76 loss: 0.0005703799\n",
      "epoch:  76 loss: 0.0005609838\n",
      "epoch:  76 loss: 0.0005586597\n",
      "epoch:  77 loss: 0.0000003265\n",
      "epoch:  77 loss: 0.0000651844\n",
      "epoch:  77 loss: 0.0000543186\n",
      "epoch:  77 loss: 0.0000479224\n",
      "epoch:  78 loss: 0.0067469548\n",
      "epoch:  78 loss: 0.0070084543\n",
      "epoch:  78 loss: 0.0071244361\n",
      "epoch:  78 loss: 0.0071787536\n",
      "epoch:  79 loss: 0.0009629349\n",
      "epoch:  79 loss: 0.0011769582\n",
      "epoch:  79 loss: 0.0012127410\n",
      "epoch:  79 loss: 0.0012246013\n",
      "epoch:  80 loss: 0.0024323489\n",
      "epoch:  80 loss: 0.0020535651\n",
      "epoch:  80 loss: 0.0030713743\n",
      "epoch:  80 loss: 0.0025462757\n",
      "epoch:  81 loss: 0.0036902954\n",
      "epoch:  81 loss: 0.0060534114\n",
      "epoch:  81 loss: 0.0027192754\n",
      "epoch:  81 loss: 0.0040484411\n",
      "epoch:  82 loss: 0.0000544918\n",
      "epoch:  82 loss: 0.0000110524\n",
      "epoch:  82 loss: 0.0004033848\n",
      "epoch:  82 loss: 0.0000223221\n",
      "epoch:  83 loss: 0.0001702548\n",
      "epoch:  83 loss: 0.0000870651\n",
      "epoch:  83 loss: 0.0000516000\n",
      "epoch:  83 loss: 0.0000314947\n",
      "epoch:  84 loss: 0.0010333447\n",
      "epoch:  84 loss: 0.0008977038\n",
      "epoch:  84 loss: 0.0008639775\n",
      "epoch:  84 loss: 0.0008532120\n",
      "epoch:  85 loss: 0.0000001975\n",
      "epoch:  85 loss: 0.0000067232\n",
      "epoch:  85 loss: 0.0000010901\n",
      "epoch:  85 loss: 0.0000016264\n",
      "epoch:  86 loss: 0.0015395392\n",
      "epoch:  86 loss: 0.0015206028\n",
      "epoch:  86 loss: 0.0015085869\n",
      "epoch:  86 loss: 0.0014999225\n",
      "epoch:  87 loss: 0.0058672517\n",
      "epoch:  87 loss: 0.0060672946\n",
      "epoch:  87 loss: 0.0061844066\n",
      "epoch:  87 loss: 0.0062687933\n",
      "epoch:  88 loss: 0.0026570307\n",
      "epoch:  88 loss: 0.0025402519\n",
      "epoch:  88 loss: 0.0024680723\n",
      "epoch:  88 loss: 0.0024148016\n",
      "epoch:  89 loss: 0.0000524253\n",
      "epoch:  89 loss: 0.0000797454\n",
      "epoch:  89 loss: 0.0000882314\n",
      "epoch:  89 loss: 0.0000890822\n",
      "epoch:  90 loss: 0.0054784617\n",
      "epoch:  90 loss: 0.0056955097\n",
      "epoch:  90 loss: 0.0057935948\n",
      "epoch:  90 loss: 0.0058536725\n",
      "epoch:  91 loss: 0.0008809245\n",
      "epoch:  91 loss: 0.0007045952\n",
      "epoch:  91 loss: 0.0006094034\n",
      "epoch:  91 loss: 0.0004902969\n",
      "epoch:  92 loss: 0.0008244665\n",
      "epoch:  92 loss: 0.0010300976\n",
      "epoch:  92 loss: 0.0011282974\n",
      "epoch:  92 loss: 0.0011800684\n",
      "epoch:  93 loss: 0.0006677325\n",
      "epoch:  93 loss: 0.0006710790\n",
      "epoch:  93 loss: 0.0006807361\n",
      "epoch:  93 loss: 0.0006899110\n",
      "epoch:  94 loss: 0.0140875597\n",
      "epoch:  94 loss: 0.0137637034\n",
      "epoch:  94 loss: 0.0128271598\n",
      "epoch:  94 loss: 0.0116729196\n",
      "epoch:  95 loss: 0.0005949730\n",
      "epoch:  95 loss: 0.0005358382\n",
      "epoch:  95 loss: 0.0004909913\n",
      "epoch:  95 loss: 0.0004558567\n",
      "epoch:  96 loss: 0.0038973740\n",
      "epoch:  96 loss: 0.0038150791\n",
      "epoch:  96 loss: 0.0037614957\n",
      "epoch:  96 loss: 0.0037102806\n",
      "epoch:  97 loss: 0.0003061242\n",
      "epoch:  97 loss: 0.0003417110\n",
      "epoch:  97 loss: 0.0003758745\n",
      "epoch:  97 loss: 0.0004063519\n",
      "epoch:  98 loss: 0.0012954045\n",
      "epoch:  98 loss: 0.0012505178\n",
      "epoch:  98 loss: 0.0012092771\n",
      "epoch:  98 loss: 0.0011689112\n",
      "epoch:  99 loss: 0.0015494666\n",
      "epoch:  99 loss: 0.0015645189\n",
      "epoch:  99 loss: 0.0016323405\n",
      "epoch:  99 loss: 0.0017117214\n",
      "epoch:  99 loss: 0.001711721416\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "for i in range(epochs):\n",
    "    batch = random.choices(list_of_tensors, k=batch_size)\n",
    "    for k in range(4):\n",
    "    \n",
    "        for seq, labels in batch:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "\n",
    "    #     if i%25 == 1:\n",
    "        \n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.12f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a60d9",
   "metadata": {},
   "source": [
    "#### Adaptation of the validation dataset to LSTM constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f1a1e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 1/5004 [00:00<00:50, 99.98it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler_price' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6297fa6e8035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavailable_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mwhole_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mavailable_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavailable_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtrain_window\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munavailable_part\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfeatures_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_inout_sequences_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhole_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mall_validation_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mall_validation_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\test\\code_f_meunier\\data_pretreatment.py\u001b[0m in \u001b[0;36mcreate_inout_sequences_validation\u001b[1;34m(input_data, tw)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_inout_sequences_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0minput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scaled_price\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler_price\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0minout_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mreal_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler_price' is not defined"
     ]
    }
   ],
   "source": [
    "from data_pretreatment import create_inout_sequences_validation\n",
    "all_validation_data = []; all_validation_target = []\n",
    "for ts in tqdm(complete_preprocessed_dataset):\n",
    "    available_part, unavailable_part = ts\n",
    "    if len(available_part) > train_window: #prediction possible\n",
    "        limit = len(available_part)\n",
    "        whole_df = pd.concat([available_part.iloc[len(available_part)-train_window:], unavailable_part])\n",
    "        features_val, target_val = create_inout_sequences_validation(whole_df, train_window)\n",
    "        all_validation_data.extend(features_val)\n",
    "        all_validation_target.extend(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors_test = [(torch.FloatTensor(np.array(df)).view(-1)) for df in all_validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbe01b",
   "metadata": {},
   "source": [
    "### Predictions on validation data --> test on single demand's day each time (no sum until departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "703cfd28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_tensors_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2125793b335e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkept_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_tensors_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_tensors_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_of_tensors_test' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_inputs = []\n",
    "kept_targets = []\n",
    "for i in tqdm(range(len(list_of_tensors_test))):\n",
    "    seq = torch.FloatTensor(list_of_tensors_test[i])\n",
    "    if len(seq)>0:\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_inputs.append(model(seq).item())\n",
    "            kept_targets.append(all_validation_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31371db1",
   "metadata": {},
   "source": [
    "#### To inverse previous scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = scaler_demand.inverse_transform(np.array(test_inputs).reshape(-1, 1))\n",
    "print(actual_predictions.shape)\n",
    "print(np.array(kept_targets).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bc46b",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1592e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"mae :\", mean_absolute_error(kept_targets, actual_predictions))\n",
    "print(\"mse :\", mean_squared_error(kept_targets, actual_predictions))\n",
    "print(\"r2 :\", r2_score(kept_targets, actual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176de0d3",
   "metadata": {},
   "source": [
    "### Conclusion on deep learning approach : \n",
    "- does not work well, no convergence (seen during the training), then performances remain bad with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cbd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
